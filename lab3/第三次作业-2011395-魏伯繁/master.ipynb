{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 加载minist数据集\n",
    "# 加载数据集这方面的工作参考了前人的工作，网页为：https://blog.csdn.net/hxxjxw/article/details/113727973\n",
    "def load_mnist(mnistdir , train):\n",
    "    ministfile = open(mnistdir,'rb')\n",
    "    ministdata = ministfile.read()\n",
    "    ministfile.close()\n",
    "    rows=1\n",
    "    cols=1\n",
    "    # 加载训练集\n",
    "    if train:\n",
    "        # 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽\n",
    "        # 因为数据结构中前4行的数据类型都是32位整型，所以采用i格式，但我们需要读取前4行数据，所以需要4个i。我们后面会看到标签集中，只使用2个ii\n",
    "        magic_num,images,rows,cols = struct.unpack_from('>iiii', ministdata,0)\n",
    "    else:\n",
    "        # 加载标签集\n",
    "        magic_num,images = struct.unpack_from('>ii', ministdata,0)\n",
    "    print('魔数:%d, 图片数量: %d张, 图片大小: %d*%d' % (magic_num, images, rows, cols))\n",
    "    # 计算加载的总像素是多少\n",
    "    size = images * rows * cols\n",
    "    # calcsize获得数据在缓存中的指针位置，从前面介绍的数据结构可以看出，读取了前4行之后，指针位置（即偏移位置offset）指向0016\n",
    "    if train:\n",
    "        pointer = struct.calcsize('>iiii')\n",
    "    else :\n",
    "        pointer =  struct.calcsize('>ii')\n",
    "    pack_data = struct.unpack_from('>' + str(size) + 'B', ministdata,pointer)\n",
    "    if train:\n",
    "        pack_data = np.reshape(pack_data,[images,rows,cols])\n",
    "    else:\n",
    "        pack_data = np.reshape(pack_data,[images])\n",
    "    # 最终返回了一个矩阵，矩阵的大小由是训练集还是标签集决定\n",
    "    # 训练集就相当于返回好多页纸，每一页纸上面有对应的行数和列数\n",
    "    print('本次解析的矩阵格式为[%d,%d,%d]' % (images,rows,cols))\n",
    "    return pack_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 在本函数中完成了本地的数据加载\n",
    "# 根据参数取出相应的验证集并做正则化\n",
    "def load_minist_data():\n",
    "    trainImages = load_mnist(\"data/train-images.idx3-ubyte\",True)\n",
    "    trainLabels = load_mnist(\"data/train-labels.idx1-ubyte\",False)\n",
    "    testImages = load_mnist(\"data/t10k-images.idx3-ubyte\",True)\n",
    "    testLabels = load_mnist(\"data/t10k-labels.idx1-ubyte\",False)\n",
    "    # 其实这里不知道为什么要pad，但还是先pad一下\n",
    "    # 我觉得可能是因为卷积的时候不丢失信息吧\n",
    "    # 对矩阵进行进行填充\n",
    "    # https://blog.csdn.net/qq_34650787/article/details/80500407\n",
    "    trainImages = np.pad(trainImages, ((0, 0), (2, 2), (2, 2)))\n",
    "    testImages = np.pad(testImages, ((0, 0), (2, 2), (2, 2)))\n",
    "\n",
    "    # 获取矩阵维度\n",
    "    real_num, real_rows, real_cols = trainImages.shape\n",
    "    real_num2,real_rows2,real_cols2 = testImages.shape\n",
    "    # print('real_rows=%d,real_cols=%d' % (real_rows,real_cols))\n",
    "\n",
    "    # 这个类型转换我也搞不太懂\n",
    "    # 而且也不知道这个reshape的目的在哪里\n",
    "    # 进行类型转换 https://blog.csdn.net/u012267725/article/details/77489244\n",
    "    # 我的理解是在这里把很多页书拼起来拼成一页，但是这一页很长\n",
    "    # print(trainImages.shape) #:这里他的输出是（60000,32,32）\n",
    "    trainImages = trainImages.astype(np.float32).reshape(real_num, 1, real_rows, real_cols)\n",
    "    testImages = testImages.astype(np.float32).reshape(real_num2, 1, real_rows, real_cols)\n",
    "    # print(trainImages.shape) #:在这里输出就变成了（60000,1,32,32）\n",
    "\n",
    "    # 接下来在训练集中划分验证集\n",
    "    # 这里的参数是可以调整的\n",
    "    varProof = -500 # 取最后的500个\n",
    "    proofImages = trainImages[varProof:]\n",
    "    proofLabels = trainLabels[varProof:]\n",
    "    trainImages = trainImages[:varProof]\n",
    "    trainLabels = trainLabels[:varProof]\n",
    "\n",
    "    # 对数据进行归一化，这里其实也可以做一个参数选项，可选可不选\n",
    "    # https://blog.csdn.net/sdgfbhgfj/article/details/123780347\n",
    "    if True:\n",
    "        mean = np.mean(trainImages,axis=0)\n",
    "    else:\n",
    "        mean = np.zeros_like(trainImages)\n",
    "    trainImages -= mean\n",
    "    proofImages -= mean\n",
    "    testImages -= mean\n",
    "    print('加载完毕')\n",
    "    # 依次返回训练数据&标签 验证数据&标签 测试数据&标签\n",
    "    return trainImages,trainLabels,proofImages,proofLabels,testImages,testLabels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 在这个cell实现随机梯度下降\n",
    "\"\"\"\n",
    "随机梯度下降算法（Stochastic gradient descent，SGD）在神经网络模型训练中，是一种很常见的优化算法。\n",
    "这个算法的流程就是在每次更新的时候使用一个样本进行梯度下降，所谓的随机二字，就是说我们可以随机用一个样本来表示所有的样本，来调整超参数θ\n",
    "程序中的参数命名来源于下面的博客：\n",
    "https://blog.csdn.net/Oscar6280868/article/details/90641638\n",
    "\"\"\"\n",
    "def sgd(theta,xj,parameters):\n",
    "    if parameters is None:\n",
    "        # 创建学习率参数\n",
    "        parameters = {'learn_rate': 1e-2}\n",
    "    theta = theta - parameters['learn_rate'] * xj\n",
    "    return theta,parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 实现一个自适应动量的随机优化方法\n",
    "\"\"\"\n",
    "论文：https://arxiv.org/pdf/1412.6980.pdf\n",
    "参数说明：\n",
    "t t：更新的步数（steps）\n",
    "α lphaα：学习率，用于控制步幅（stepsize）\n",
    "θ hetaθ：要求解（更新）的参数\n",
    "β1 一阶矩衰减系数\n",
    "β2 二阶矩衰减系数\n",
    "f（θ） 目标函数\n",
    "g 目标函数对θ求导所得梯度\n",
    "m 梯度g的一阶矩\n",
    "v 梯度g的二阶矩\n",
    "\n",
    "参考的博客：https://blog.csdn.net/sinat_36618660/article/details/100026261\n",
    "\"\"\"\n",
    "def adam(theta,xj,parameters):\n",
    "    if parameters is None:\n",
    "        parameters={}\n",
    "    parameters.setdefault(\"learn_rate\", 1e-3)\n",
    "    parameters.setdefault(\"beta1\", 0.9)\n",
    "    parameters.setdefault(\"beta2\", 0.999)\n",
    "    parameters.setdefault(\"e\", 1e-8)\n",
    "    parameters.setdefault(\"m\", np.zeros_like(theta))\n",
    "    parameters.setdefault(\"v\", np.zeros_like(theta))\n",
    "    parameters.setdefault(\"t\", 0)\n",
    "\n",
    "\n",
    "    b1=parameters['beta1']\n",
    "    b2=parameters['beta2']\n",
    "    parameters['t'] = parameters['t'] + 1\n",
    "    parameters['m'] = ((1 - b1) * xj) + (b1 * parameters['m'])\n",
    "    parameters['v'] = ((1 - b2) * (xj ** 2)) + (b2 * parameters['v'])\n",
    "    temp1 = 1 - (b1 ** parameters['t'])\n",
    "    temp2 = 1 - (b2 ** parameters['t'])\n",
    "    new_theta = theta - parameters['learn_rate'] * (parameters['m'] / temp1) / (np.sqrt(parameters['v'] / temp2) + parameters['e'])\n",
    "    return new_theta,parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "class Train:\n",
    "    \"\"\"\n",
    "    Lenet5 相当于传进来一个类，这个类保存了一次训练需要的信息\n",
    "    data\n",
    "    epoch 所有样本都跑过一遍所需次数\n",
    "    func 选择哪个参数优化函数\n",
    "    parameter\n",
    "    decrease 学习率下降数\n",
    "    batchSize 每次只使用数据集中的部分样本\n",
    "    exp\n",
    "    \"\"\"\n",
    "    # 这里其实可以初始化一下\n",
    "    def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\n",
    "        self.model = Lenet5\n",
    "        self.train1 = data[\"train1\"] #在传进来data前也得注意\n",
    "        self.train2 = data[\"train2\"]\n",
    "        self.label1 = data[\"label1\"]\n",
    "        self.label2 = data[\"label2\"]\n",
    "        self.func = func #让他指向了指定的那个函数\n",
    "        self.learn_rate_decay = decrease #学习率下降\n",
    "        self.batchSize = batchSize #一次使用的样本数\n",
    "        self.exp = exp\n",
    "        self.epoch = epoch\n",
    "\n",
    "        # 保存最高准确率以及最佳参数\n",
    "        self.bestAccurate = 0\n",
    "        self.bestPara = {}\n",
    "\n",
    "        # 为了绘图\n",
    "        self.losses = []\n",
    "        self.TrainAccurates = []\n",
    "        self.ProofAccurates = []\n",
    "\n",
    "        # 按照他的意思这个东西是深度拷贝\n",
    "        self.funcParameter = {}\n",
    "        for i in self.model.para:\n",
    "            d = {k: v for k,v in parameter.items()}\n",
    "            self.funcParameter[i] = d\n",
    "\n",
    "    # 直接返回向量\n",
    "    def calAccuracy(self,train,label,batchSize):\n",
    "        # 需要做几轮运算\n",
    "        myRound = math.ceil(train.shape[0] / batchSize)\n",
    "        # 最终的预测结果\n",
    "        prediction = []\n",
    "        for i in range(myRound):\n",
    "            start = i*batchSize\n",
    "            end = (i+1)*batchSize\n",
    "            # 这个不太懂loss算了个什么\n",
    "            # 为什么只用给train就行了，还有就是train是什么，最后数组保存的是什么\n",
    "            # 实际loss返回两个值\n",
    "            acc = self.model.spread2(train[start:end]) #计算损失\n",
    "            p = np.argmax(acc,axis=1) #找出最大值\n",
    "            prediction.append(p)\n",
    "        prediction = np.hstack(prediction) #横向拼接向量\n",
    "        return prediction\n",
    "\n",
    "    def calAccuracy2(self,train,label,batchSize):\n",
    "        prediction = self.calAccuracy(train, label, batchSize)\n",
    "        return np.mean(prediction == label)\n",
    "\n",
    "    def myTrain(self):\n",
    "        # 为了绘图 把这几个先初始化为空\n",
    "        self.losses = []\n",
    "        self.TrainAccurates = []\n",
    "        self.ProofAccurates = []\n",
    "        # 按照不同的轮以对应的batchsize训练\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(math.ceil(self.train1.shape[0] / self.batchSize)-1):\n",
    "                start = j * self.batchSize\n",
    "                end = (j+1) * self.batchSize\n",
    "                arr = np.arange(start,end)\n",
    "                batch_train = self.train1[arr]\n",
    "                batch_label = self.label1[arr]\n",
    "                # 我觉得在算loss的时候就更新了model的params\n",
    "                loss,res = self.model.spread(batch_train,batch_label)\n",
    "                self.losses.append(loss)\n",
    "                for k,v in self.model.para.items():\n",
    "                    if self.func =='sgd':\n",
    "                        self.model.para[k],self.funcParameter[k] = sgd(v,res[k],self.funcParameter[k])\n",
    "                    else:\n",
    "                        self.model.para[k],self.funcParameter[k] = adam(v,res[k],self.funcParameter[k])\n",
    "                print('第 %d 轮第 %d 个batch中loss为 %f' % (i+1,j+1,loss))\n",
    "\n",
    "            a = self.calAccuracy2(self.train1,self.label1,100)\n",
    "            b = self.calAccuracy2(self.train2,self.label2,100)\n",
    "            self.TrainAccurates.append(a)\n",
    "            self.ProofAccurates.append(b)\n",
    "\n",
    "            print('第 %d 次迭代，训练集正确率 %f 验证集正确率 %f' % (i+1,a,b))\n",
    "\n",
    "            if b > self.bestAccurate :\n",
    "                self.bestAccurate = b\n",
    "                self.bestPara.clear() #更新自己参数里的最佳参数\n",
    "                for k,v in self.model.para.items():\n",
    "                    self.bestPara[k] = v.copy()\n",
    "\n",
    "            if i+1 == self.epoch:\n",
    "                filename = \"TrainingResult\"\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump(self.model,f)\n",
    "                    print(\"成功保存模型\")\n",
    "\n",
    "            if i > 10:\n",
    "                for q in self.funcParameter:\n",
    "                    self.funcParameter[q]['learn_rate'] *= self.learn_rate_decay\n",
    "\n",
    "        # 我觉得这里最后一轮出最佳的话就保存不了了啊\n",
    "        self.model.params = self.bestPara"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 卷积层前向传播的过程\n",
    "def convolution_spread_forward(train, convolution_entity, bias, para):\n",
    "    pic_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    convolution_num = convolution_entity.shape[0]\n",
    "    convolution_height = convolution_entity.shape[2]\n",
    "    convolution_wide = convolution_entity.shape[3]\n",
    "    step = para['step'] #这里代表着步长\n",
    "    padding = para['pad'] # 填充部分的大小\n",
    "    new_train = np.pad(train,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    loop_outside  = height + 2 * padding - convolution_height\n",
    "    loop_outside  = loop_outside // step\n",
    "    loop_outside  = loop_outside + 1\n",
    "    loop_inside = wide + 2 * padding - convolution_wide\n",
    "    loop_inside = loop_inside // step\n",
    "    loop_inside = loop_inside + 1\n",
    "    weight = np.zeros((pic_num, convolution_num, loop_outside, loop_inside))\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = []\n",
    "\n",
    "            temp = new_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide]\n",
    "            temp = temp.reshape(pic_num,1,channel,convolution_height,convolution_wide)\n",
    "            new_convolution = convolution_entity.reshape(1,convolution_num,channel,convolution_height,convolution_wide)\n",
    "            weight[:,:,i,j] = np.sum(temp * new_convolution,axis=(-3,-2,-1))\n",
    "            weight[:,:,i,j] += bias\n",
    "    package = (train,convolution_entity,bias,para)\n",
    "    return weight,package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def convolution_spread_backward(derivation,pack):\n",
    "    num = derivation.shape[0]\n",
    "    convolution_num = derivation.shape[1]\n",
    "    loop_outside = derivation.shape[2]\n",
    "    loop_inside = derivation.shape[3]\n",
    "    train = pack[0]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    convolution = pack[1]\n",
    "    derivation_convolution = np.zeros_like(convolution)\n",
    "    channel =convolution.shape[1]\n",
    "    convolution_height = convolution.shape[2]\n",
    "    convolution_wide = convolution.shape[3]\n",
    "    bias = pack[2]\n",
    "    derivation_bias = np.zeros_like(bias)\n",
    "    para = pack[3]\n",
    "    padding = para['pad']\n",
    "    step = para['step']\n",
    "    new_train = np.pad(train,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    derivation_train = np.zeros_like(new_train)\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = derivation[:,:,i,j]\n",
    "            temp = temp.reshape((num,1,1,1,convolution_num))\n",
    "            temp2 = convolution.transpose((1,2,3,0))\n",
    "            temp2 = temp2.reshape((1,channel,convolution_height,convolution_wide,convolution_num))\n",
    "            temp_sum = np.sum(temp*temp2,axis=-1)\n",
    "            derivation_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide] +=temp_sum\n",
    "            temp3 = derivation[:,:,i,j].T\n",
    "            temp3 = temp3.reshape((convolution_num,1,1,1,num))\n",
    "            temp4 = new_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide].transpose(1,2,3,0)\n",
    "            temp_sum = np.sum(temp3*temp4,axis=-1)\n",
    "            derivation_convolution += temp_sum\n",
    "            temp_sum = np.sum(derivation[:,:,i,j],axis=0)\n",
    "            derivation_bias += temp_sum\n",
    "\n",
    "    derivation_weight = []\n",
    "    derivation_weight = derivation_train[:,:,padding:padding+height,padding:padding+wide]\n",
    "    return derivation_weight,derivation_convolution,derivation_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def relu_spread_froward(Input):\n",
    "    # 由于老师在课堂上强调sigmoid的梯度消失问题\n",
    "    # 解决办法可以为替换激活函数\n",
    "    # 使用relu作为激活函数\n",
    "    temp = np.maximum(0,Input)\n",
    "    return temp,Input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def relu_spread_backward(derivation,Input):\n",
    "    # relu函数的反向传播函数\n",
    "    temp = Input\n",
    "    temp[temp>0]=1\n",
    "    temp[temp<=0]=0\n",
    "    derivation_train = derivation * Input\n",
    "    return derivation_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def pool_spread_forward(train,para):\n",
    "    pool_height = para['pool_height'] #传入的时候也要按照这个格式命名\n",
    "    pool_wide = para['pool_wide']\n",
    "    step = para['step']\n",
    "    train_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    loop_outside = height - pool_height\n",
    "    loop_outside = loop_outside // step\n",
    "    loop_outside = loop_outside + 1\n",
    "    loop_inside = wide - pool_wide\n",
    "    loop_inside = loop_inside // step\n",
    "    loop_inside = loop_inside + 1\n",
    "    ret = np.zeros(shape=(train_num,channel,loop_outside,loop_inside))\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = train[:,:,i*step:i*step+pool_height,j*step:j*step+pool_wide]\n",
    "            ret[:,:,i,j]=np.max(temp,axis=(2,3))\n",
    "    pack = (train,para)\n",
    "    return ret,pack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def pool_spread_backward(derivation,pack):\n",
    "    train = pack[0]\n",
    "    derivation_train = np.zeros_like(train)\n",
    "    train_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    para = pack[1]\n",
    "    pool_height = para['pool_height']\n",
    "    pool_wide = para['pool_wide']\n",
    "    step = para['step']\n",
    "    loop_outside = height - pool_height\n",
    "    loop_outside = loop_outside // step\n",
    "    loop_outside = loop_outside + 1\n",
    "    loop_inside = wide - pool_wide\n",
    "    loop_inside = loop_inside // step\n",
    "    loop_inside = loop_inside + 1\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = train[:,:,i*step:i*step+pool_height,j*step:j*step+pool_wide]\n",
    "            temp = temp.reshape(train_num,channel,-1)\n",
    "            tempMax = np.argmax(temp,axis=-1)\n",
    "            temp_max = np.unravel_index(tempMax,(pool_height,pool_wide))\n",
    "            which = np.array(temp_max)\n",
    "            for k in range(train_num):\n",
    "                for l in range(channel):\n",
    "                    temp2 = derivation[k][l][i][j]\n",
    "                    index_3 = i*step+which[0][k][l]\n",
    "                    index_4 = j*step+which[1][k][l]\n",
    "                    derivation_train[k][l][index_3][index_4] += temp2\n",
    "\n",
    "    return derivation_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def full_connect_spread_forward(train,weight,bias):\n",
    "    # 全连接层的前向传播\n",
    "    index_0 = train.shape[0]\n",
    "    temp = train.reshape(index_0,-1)\n",
    "    temp = temp.dot(weight) + bias\n",
    "    pack = (train,weight,bias)\n",
    "    return temp,pack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def full_connect_spread_backward(derivation,pack):\n",
    "    train = pack[0]\n",
    "    index_0 = train.shape[0]\n",
    "    weight = pack[1]\n",
    "    weight_T = weight.T\n",
    "    bias = pack[2]\n",
    "    derivation_train = derivation.dot(weight_T)\n",
    "    derivation_train = derivation_train.reshape(train.shape)\n",
    "    temp = train.reshape(index_0,-1)\n",
    "    temp_T = temp.T\n",
    "    derivation_weight = temp_T.dot(derivation)\n",
    "    derivation_bias = np.sum(derivation,axis=0)\n",
    "    return derivation_train,derivation_weight,derivation_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def softmax(Input,Output):\n",
    "    # softmax损失函数的计算\n",
    "    data_num = Input.shape[0]\n",
    "    cal_e = np.exp(Input)\n",
    "    cal_sum = np.sum(cal_e,axis=1)\n",
    "    cal_log = np.log(cal_sum)\n",
    "    temp = Input[range(data_num),list(Output)]\n",
    "    ret = np.mean(cal_log-temp)\n",
    "    temp_sum = cal_sum.reshape(data_num,1)\n",
    "    derivation_Input = cal_e / temp_sum\n",
    "    derivation_Input[range(data_num),list(Output)] = derivation_Input[range(data_num),list(Output)]-1\n",
    "    derivation_Input = derivation_Input / data_num\n",
    "    return ret,derivation_Input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class MyLenet5:\n",
    "    def __init__(self):\n",
    "        # 输入的图像通道为1\n",
    "        # 输入图像大小为32*32\n",
    "        self.input_channel = 1\n",
    "        self.input_height = 32\n",
    "        self.input_wide = 32\n",
    "        # 第一次卷积核有6个\n",
    "        # 第二次卷积核有16个\n",
    "        # 卷积核大小都是5*5\n",
    "        self.convolution_num1 = 6\n",
    "        self.convolution_num2 = 16\n",
    "        self.convolution_height = 5\n",
    "        self.convolution_wide =5\n",
    "        # 全连接层的大小\n",
    "        self.full_connect1 = 400\n",
    "        self.full_connect2 = 120\n",
    "        self.full_connect3 = 84\n",
    "        # 输出类型个数\n",
    "        self.output_size = 10\n",
    "        # lenet5的参数\n",
    "        # 卷积池化层的参数和偏置项\n",
    "        # 全连接层的参数和偏置项\n",
    "        self.para = {}\n",
    "        # 正则项\n",
    "        self.regularItem = 0.001\n",
    "        self.datatype = np.float32\n",
    "        # 生成时必要的标准差\n",
    "        self.standard_deviation = 0.001\n",
    "\n",
    "        #首先初始化第一个卷积层\n",
    "        convolution1_para_weight = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale=self.standard_deviation,\n",
    "            size=(self.convolution_num1,self.input_channel,self.convolution_height,self.convolution_wide)\n",
    "        )\n",
    "        convolution1_para_bias = np.zeros(self.convolution_num1)\n",
    "\n",
    "        #接下来初始化第二个卷积层\n",
    "        convolution2_para_weight = np.random.normal(\n",
    "            loc = 0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.convolution_num2,self.convolution_num1,self.convolution_height,self.convolution_wide)\n",
    "        )\n",
    "        convolution2_para_bias = np.zeros(self.convolution_num2)\n",
    "\n",
    "        #接下来初始化第一个全连接层\n",
    "        full_connect1_para_weight = np.random.normal(\n",
    "            loc = 0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.full_connect1,self.full_connect2)\n",
    "        )\n",
    "        full_connect1_para_bias = np.zeros(self.full_connect2)\n",
    "\n",
    "        #接下来初始化第二个全连接层\n",
    "        full_connect2_para_weight = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.full_connect2,self.full_connect3)\n",
    "        )\n",
    "        full_connect2_para_bias = np.zeros(self.full_connect3)\n",
    "\n",
    "        #接下来初始化第三个全连接层\n",
    "        full_connect3_para_weight = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.full_connect3,self.output_size)\n",
    "        )\n",
    "        full_connect3_para_bias = np.zeros(self.output_size)\n",
    "\n",
    "        self.para['convolution1_para_weight'] = convolution1_para_weight\n",
    "        self.para['convolution1_para_bias'] = convolution1_para_bias\n",
    "        self.para['convolution2_para_weight'] = convolution2_para_weight\n",
    "        self.para['convolution2_para_bias'] = convolution2_para_bias\n",
    "        self.para['full_connect1_para_weight'] = full_connect1_para_weight\n",
    "        self.para['full_connect1_para_bias'] = full_connect1_para_bias\n",
    "        self.para['full_connect2_para_weight'] = full_connect2_para_weight\n",
    "        self.para['full_connect2_para_bias'] = full_connect2_para_bias\n",
    "        self.para['full_connect3_para_weight'] = full_connect3_para_weight\n",
    "        self.para['full_connect3_para_bias'] = full_connect3_para_bias\n",
    "\n",
    "        self.changeType()\n",
    "\n",
    "    def changeType(self):\n",
    "        for k,v in self.para.items():\n",
    "            self.para[k] = v.astype(np.float32)\n",
    "\n",
    "    # 这个在不同的层那里要改\n",
    "    def spread(self,train,label):\n",
    "        # 卷积层步长为1，没有padding\n",
    "        convolution_para = {}\n",
    "        convolution_para.clear()\n",
    "        convolution_para['step']=1\n",
    "        convolution_para['pad']=0\n",
    "\n",
    "        # 池化层步长为2 范围就是【2,2】\n",
    "        pooling_para={}\n",
    "        pooling_para.clear()\n",
    "        pooling_para['step']=2\n",
    "        pooling_para['pool_height']=2\n",
    "        pooling_para['pool_wide']=2\n",
    "\n",
    "        # 拿出所有需要的权重\n",
    "        c1w = self.para['convolution1_para_weight']\n",
    "        temp1 = np.sum(c1w*c1w)\n",
    "        c1b = self.para['convolution1_para_bias']\n",
    "        c2w = self.para['convolution2_para_weight']\n",
    "        temp2 = np.sum(c2w*c2w)\n",
    "        c2b = self.para['convolution2_para_bias']\n",
    "        fc3w = self.para['full_connect1_para_weight']\n",
    "        temp3 = np.sum(fc3w*fc3w)\n",
    "        fc3b = self.para['full_connect1_para_bias']\n",
    "        fc4w = self.para['full_connect2_para_weight']\n",
    "        temp4 = np.sum(fc4w*fc4w)\n",
    "        fc4b = self.para['full_connect2_para_bias']\n",
    "        fc5w = self.para['full_connect3_para_weight']\n",
    "        temp5 = np.sum(fc5w*fc5w)\n",
    "        fc5b = self.para['full_connect3_para_bias']\n",
    "\n",
    "        cal_temp = temp1 + temp2 +temp3 +temp4 +temp5\n",
    "        cal_temp = (self.regularItem*cal_temp)/2\n",
    "        #开始前向传播\n",
    "\n",
    "        # 第一次卷积层+relu+第一层池化层\n",
    "        c1_res,c1_pack = convolution_spread_forward(train,c1w,c1b,convolution_para)\n",
    "\n",
    "        c1_relu_res,c1_relu_pack = relu_spread_froward(c1_res)\n",
    "\n",
    "        c1_pool_res,c1_pool_pack = pool_spread_forward(c1_relu_res,pooling_para)\n",
    "\n",
    "        # 第二次卷积层+relu+第二层池化层\n",
    "        c2_res,c2_pack = convolution_spread_forward(c1_pool_res,c2w,c2b,convolution_para)\n",
    "\n",
    "        c2_relu_res,c2_relu_pack = relu_spread_froward(c2_res)\n",
    "\n",
    "        c2_pool_res,c2_pool_pack = pool_spread_forward(c2_relu_res,pooling_para)\n",
    "\n",
    "        # 第一层全连接层+relu层\n",
    "        c3_res,c3_pack = full_connect_spread_forward(c2_pool_res,fc3w,fc3b)\n",
    "\n",
    "        c3_relu_res,c3_relu_pack = relu_spread_froward(c3_res)\n",
    "\n",
    "        # 第二层全连接层\n",
    "        c4_res,c4_pack = full_connect_spread_forward(c3_relu_res,fc4w,fc4b)\n",
    "\n",
    "        c4_relu_res,c4_relu_pack = relu_spread_froward(c4_res)\n",
    "\n",
    "        # 输出层\n",
    "        c5_out,c5_pack = full_connect_spread_forward(c4_relu_res,fc5w,fc5b)\n",
    "\n",
    "        # 开始反向传播\n",
    "\n",
    "        # 声明校正值为0.0\n",
    "        correction1 = 0.0\n",
    "        correction2 = 0.0\n",
    "        correction3 = 0.0\n",
    "        correction4 = 0.0\n",
    "        correction5 = 0.0\n",
    "        correction1_2 = 0.0\n",
    "        correction2_2 = 0.0\n",
    "        correction3_2 = 0.0\n",
    "        correction4_2 = 0.0\n",
    "        correction5_2 = 0.0\n",
    "\n",
    "\n",
    "        l,derivation_train = softmax(c5_out,label)\n",
    "\n",
    "        l = l + cal_temp\n",
    "\n",
    "        derivation_train,correction5,correction5_2 = full_connect_spread_backward(derivation_train,c5_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c4_relu_pack)\n",
    "        correction5 =correction5+self.regularItem*self.para['full_connect3_para_weight']\n",
    "\n",
    "        derivation_train,correction4,correction4_2 = full_connect_spread_backward(derivation_train,c4_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c3_relu_pack)\n",
    "        correction4 =correction4+self.regularItem*self.para['full_connect2_para_weight']\n",
    "\n",
    "        derivation_train,correction3,correction3_2 = full_connect_spread_backward(derivation_train,c3_pack)\n",
    "        derivation_train = pool_spread_backward(derivation_train,c2_pool_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c2_relu_pack)\n",
    "        correction3 =correction3+self.regularItem*self.para['full_connect1_para_weight']\n",
    "\n",
    "        derivation_train,correction2,correction2_2 = convolution_spread_backward(derivation_train,c2_pack)\n",
    "        derivation_train = pool_spread_backward(derivation_train,c1_pool_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c1_relu_pack)\n",
    "        correction2 =correction2+self.regularItem*self.para['convolution2_para_weight']\n",
    "\n",
    "        derivation_train,correction1,correction1_2 = convolution_spread_backward(derivation_train,c1_pack)\n",
    "        correction1 =correction1+self.regularItem*self.para['convolution1_para_weight']\n",
    "\n",
    "        corr = {}\n",
    "        corr['convolution1_para_weight'] = correction1\n",
    "        corr['convolution1_para_bias'] = correction1_2\n",
    "\n",
    "        corr['convolution2_para_weight'] = correction2\n",
    "        corr['convolution2_para_bias'] = correction2_2\n",
    "\n",
    "        corr['full_connect1_para_weight'] = correction3\n",
    "        corr['full_connect1_para_bias'] = correction3_2\n",
    "\n",
    "        corr['full_connect2_para_weight'] = correction4\n",
    "        corr['full_connect2_para_bias'] = correction4_2\n",
    "\n",
    "        corr['full_connect3_para_weight'] = correction5\n",
    "        corr['full_connect3_para_bias'] = correction5_2\n",
    "\n",
    "        return l,corr\n",
    "\n",
    "    def spread2(self,train):\n",
    "        # 卷积层步长为1，没有padding\n",
    "        convolution_para = {}\n",
    "        convolution_para.clear()\n",
    "        convolution_para['step']=1\n",
    "        convolution_para['pad']=0\n",
    "\n",
    "        # 池化层步长为2 范围就是【2,2】\n",
    "        pooling_para={}\n",
    "        pooling_para.clear()\n",
    "        pooling_para['step']=2\n",
    "        pooling_para['pool_height']=2\n",
    "        pooling_para['pool_wide']=2\n",
    "\n",
    "        # 拿出所有需要的权重\n",
    "        c1w = self.para['convolution1_para_weight']\n",
    "        temp1 = np.sum(c1w*c1w)\n",
    "        c1b = self.para['convolution1_para_bias']\n",
    "        c2w = self.para['convolution2_para_weight']\n",
    "        temp2 = np.sum(c2w*c2w)\n",
    "        c2b = self.para['convolution2_para_bias']\n",
    "        fc3w = self.para['full_connect1_para_weight']\n",
    "        temp3 = np.sum(fc3w*fc3w)\n",
    "        fc3b = self.para['full_connect1_para_bias']\n",
    "        fc4w = self.para['full_connect2_para_weight']\n",
    "        temp4 = np.sum(fc4w*fc4w)\n",
    "        fc4b = self.para['full_connect2_para_bias']\n",
    "        fc5w = self.para['full_connect3_para_weight']\n",
    "        temp5 = np.sum(fc5w*fc5w)\n",
    "        fc5b = self.para['full_connect3_para_bias']\n",
    "\n",
    "        cal_temp = temp1 + temp2 +temp3 +temp4 +temp5\n",
    "        cal_temp = (self.regularItem*cal_temp)/2\n",
    "        #开始前向传播\n",
    "\n",
    "        # 第一次卷积层+relu+第一层池化层\n",
    "        c1_res,c1_pack = convolution_spread_forward(train,c1w,c1b,convolution_para)\n",
    "\n",
    "        c1_relu_res,c1_relu_pack = relu_spread_froward(c1_res)\n",
    "\n",
    "        c1_pool_res,c1_pool_pack = pool_spread_forward(c1_relu_res,pooling_para)\n",
    "\n",
    "        # 第二次卷积层+relu+第二层池化层\n",
    "        c2_res,c2_pack = convolution_spread_forward(c1_pool_res,c2w,c2b,convolution_para)\n",
    "\n",
    "        c2_relu_res,c2_relu_pack = relu_spread_froward(c2_res)\n",
    "\n",
    "        c2_pool_res,c2_pool_pack = pool_spread_forward(c2_relu_res,pooling_para)\n",
    "\n",
    "        # 第一层全连接层+relu层\n",
    "        c3_res,c3_pack = full_connect_spread_forward(c2_pool_res,fc3w,fc3b)\n",
    "\n",
    "        c3_relu_res,c3_relu_pack = relu_spread_froward(c3_res)\n",
    "\n",
    "        # 第二层全连接层\n",
    "        c4_res,c4_pack = full_connect_spread_forward(c3_relu_res,fc4w,fc4b)\n",
    "\n",
    "        c4_relu_res,c4_relu_pack = relu_spread_froward(c4_res)\n",
    "\n",
    "        # 输出层\n",
    "        c5_out,c5_pack = full_connect_spread_forward(c4_relu_res,fc5w,fc5b)\n",
    "\n",
    "        return c5_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "魔数:2051, 图片数量: 60000张, 图片大小: 28*28\n",
      "本次解析的矩阵格式为[60000,28,28]\n",
      "魔数:2049, 图片数量: 60000张, 图片大小: 1*1\n",
      "本次解析的矩阵格式为[60000,1,1]\n",
      "魔数:2051, 图片数量: 10000张, 图片大小: 28*28\n",
      "本次解析的矩阵格式为[10000,28,28]\n",
      "魔数:2049, 图片数量: 10000张, 图片大小: 1*1\n",
      "本次解析的矩阵格式为[10000,1,1]\n",
      "加载完毕\n",
      "(59500, 1, 32, 32)\n",
      "(59500,)\n",
      "(500, 1, 32, 32)\n",
      "(500,)\n",
      "第 1 轮第 1 个batch中loss为 2.302616\n",
      "第 1 轮第 2 个batch中loss为 2.302496\n",
      "第 1 轮第 3 个batch中loss为 2.302552\n",
      "第 1 轮第 4 个batch中loss为 2.302674\n",
      "第 1 轮第 5 个batch中loss为 2.302406\n",
      "第 1 轮第 6 个batch中loss为 2.301984\n",
      "第 1 轮第 7 个batch中loss为 2.302518\n",
      "第 1 轮第 8 个batch中loss为 2.302697\n",
      "第 1 轮第 9 个batch中loss为 2.302375\n",
      "第 1 轮第 10 个batch中loss为 2.302431\n",
      "第 1 轮第 11 个batch中loss为 2.302577\n",
      "第 1 轮第 12 个batch中loss为 2.302395\n",
      "第 1 轮第 13 个batch中loss为 2.302340\n",
      "第 1 轮第 14 个batch中loss为 2.302123\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\u001B[39;00m\n\u001B[0;32m     20\u001B[0m myTrain \u001B[38;5;241m=\u001B[39m Train(Lenet5\u001B[38;5;241m=\u001B[39mTrainModel,data\u001B[38;5;241m=\u001B[39mdata,epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m,func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m,parameter\u001B[38;5;241m=\u001B[39mmyParameter,decrease\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.75\u001B[39m,batchSize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m,exp\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m \u001B[43mmyTrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmyTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mTrain.myTrain\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     74\u001B[0m batch_label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel1[arr]\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# 我觉得在算loss的时候就更新了model的params\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m loss,res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_label\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k,v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpara\u001B[38;5;241m.\u001B[39mitems():\n",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36mMyLenet5.spread\u001B[1;34m(self, train, label)\u001B[0m\n\u001B[0;32m    181\u001B[0m correction3 \u001B[38;5;241m=\u001B[39mcorrection3\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregularItem\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpara[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfull_connect1_para_weight\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    183\u001B[0m derivation_train,correction2,correction2_2 \u001B[38;5;241m=\u001B[39m convolution_spread_backward(derivation_train,c2_pack)\n\u001B[1;32m--> 184\u001B[0m derivation_train \u001B[38;5;241m=\u001B[39m \u001B[43mpool_spread_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mderivation_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43mc1_pool_pack\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m derivation_train \u001B[38;5;241m=\u001B[39m relu_spread_backward(derivation_train,c1_relu_pack)\n\u001B[0;32m    186\u001B[0m correction2 \u001B[38;5;241m=\u001B[39mcorrection2\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregularItem\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpara[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconvolution2_para_weight\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36mpool_spread_backward\u001B[1;34m(derivation, pack)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(loop_inside):\n\u001B[0;32m     20\u001B[0m     temp \u001B[38;5;241m=\u001B[39m train[:,:,i\u001B[38;5;241m*\u001B[39mstep:i\u001B[38;5;241m*\u001B[39mstep\u001B[38;5;241m+\u001B[39mpool_height,j\u001B[38;5;241m*\u001B[39mstep:j\u001B[38;5;241m*\u001B[39mstep\u001B[38;5;241m+\u001B[39mpool_wide]\n\u001B[1;32m---> 21\u001B[0m     temp \u001B[38;5;241m=\u001B[39m \u001B[43mtemp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_num\u001B[49m\u001B[43m,\u001B[49m\u001B[43mchannel\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     tempMax \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(temp,axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     23\u001B[0m     temp_max \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munravel_index(tempMax,(pool_height,pool_wide))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 调用接口，完成数据加载并打包\n",
    "trainImages,trainLabels,proofImages,proofLabels,testImages,testLabels = load_minist_data()\n",
    "data = {}\n",
    "data['train1']=trainImages\n",
    "data['train2']=proofImages\n",
    "data['label1']=trainLabels\n",
    "data['label2']=proofLabels\n",
    "data['test_image']=testImages\n",
    "data['test_label']=testLabels\n",
    "print(data['train1'].shape)\n",
    "print(data['label1'].shape)\n",
    "print(data['train2'].shape)\n",
    "print(data['label2'].shape)\n",
    "\n",
    "\n",
    "TrainModel = MyLenet5()\n",
    "myParameter = {}\n",
    "myParameter['learn_rate']=1e-3\n",
    "# def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\n",
    "myTrain = Train(Lenet5=TrainModel,data=data,epoch=20,func='adam',parameter=myParameter,decrease=0.75,batchSize=200,exp=1)\n",
    "myTrain.myTrain()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Test accuracy:\",\n",
    "    myTrain.calAccuracy2(data['test_image'], data['test_label'],100)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=100, figsize=(20, 6))\n",
    "plt.plot(myTrain.losses, 'o', markersize = 2)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(\"1.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(dpi=100, figsize=(20, 6))\n",
    "plt.plot(myTrain.TrainAccurates, '-o')\n",
    "plt.plot(myTrain.ProofAccurates, '-o')\n",
    "plt.legend(['TrainAccurates', 'ProofAccurates'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig(\"2.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
