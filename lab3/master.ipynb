{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# 加载minist数据集\n",
    "# 加载数据集这方面的工作参考了前人的工作，网页为：https://blog.csdn.net/hxxjxw/article/details/113727973\n",
    "def load_mnist(mnistdir , train):\n",
    "    ministfile = open(mnistdir,'rb')\n",
    "    ministdata = ministfile.read()\n",
    "    ministfile.close()\n",
    "    rows=1\n",
    "    cols=1\n",
    "    # 加载训练集\n",
    "    if train:\n",
    "        # 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽\n",
    "        # 因为数据结构中前4行的数据类型都是32位整型，所以采用i格式，但我们需要读取前4行数据，所以需要4个i。我们后面会看到标签集中，只使用2个ii\n",
    "        magic_num,images,rows,cols = struct.unpack_from('>iiii', ministdata,0)\n",
    "    else:\n",
    "        # 加载标签集\n",
    "        magic_num,images = struct.unpack_from('>ii', ministdata,0)\n",
    "    print('魔数:%d, 图片数量: %d张, 图片大小: %d*%d' % (magic_num, images, rows, cols))\n",
    "    # 计算加载的总像素是多少\n",
    "    size = images * rows * cols\n",
    "    # calcsize获得数据在缓存中的指针位置，从前面介绍的数据结构可以看出，读取了前4行之后，指针位置（即偏移位置offset）指向0016\n",
    "    if train:\n",
    "        pointer = struct.calcsize('>iiii')\n",
    "    else :\n",
    "        pointer =  struct.calcsize('>ii')\n",
    "    pack_data = struct.unpack_from('>' + str(size) + 'B', ministdata,pointer)\n",
    "    if train:\n",
    "        pack_data = np.reshape(pack_data,[images,rows,cols])\n",
    "    else:\n",
    "        pack_data = np.reshape(pack_data,[images])\n",
    "    # 最终返回了一个矩阵，矩阵的大小由是训练集还是标签集决定\n",
    "    # 训练集就相当于返回好多页纸，每一页纸上面有对应的行数和列数\n",
    "    print('本次解析的矩阵格式为[%d,%d,%d]' % (images,rows,cols))\n",
    "    return pack_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# 在本函数中完成了本地的数据加载\n",
    "# 根据参数取出相应的验证集并做正则化\n",
    "def load_minist_data():\n",
    "    trainImages = load_mnist(\"data/train-images.idx3-ubyte\",True)\n",
    "    trainLabels = load_mnist(\"data/train-labels.idx1-ubyte\",False)\n",
    "    testImages = load_mnist(\"data/t10k-images.idx3-ubyte\",True)\n",
    "    testLabels = load_mnist(\"data/t10k-labels.idx1-ubyte\",False)\n",
    "    # 其实这里不知道为什么要pad，但还是先pad一下\n",
    "    # 我觉得可能是因为卷积的时候不丢失信息吧\n",
    "    # 对矩阵进行进行填充\n",
    "    # https://blog.csdn.net/qq_34650787/article/details/80500407\n",
    "    trainImages = np.pad(trainImages, ((0, 0), (2, 2), (2, 2)))\n",
    "    testImages = np.pad(testImages, ((0, 0), (2, 2), (2, 2)))\n",
    "\n",
    "    # 获取矩阵维度\n",
    "    real_num, real_rows, real_cols = trainImages.shape\n",
    "    real_num2,real_rows2,real_cols2 = testImages.shape\n",
    "    # print('real_rows=%d,real_cols=%d' % (real_rows,real_cols))\n",
    "\n",
    "    # 这个类型转换我也搞不太懂\n",
    "    # 而且也不知道这个reshape的目的在哪里\n",
    "    # 进行类型转换 https://blog.csdn.net/u012267725/article/details/77489244\n",
    "    # 我的理解是在这里把很多页书拼起来拼成一页，但是这一页很长\n",
    "    # print(trainImages.shape) #:这里他的输出是（60000,32,32）\n",
    "    trainImages = trainImages.astype(np.float32).reshape(real_num, 1, real_rows, real_cols)\n",
    "    testImages = testImages.astype(np.float32).reshape(real_num2, 1, real_rows, real_cols)\n",
    "    # print(trainImages.shape) #:在这里输出就变成了（60000,1,32,32）\n",
    "\n",
    "    # 接下来在训练集中划分验证集\n",
    "    # 这里的参数是可以调整的\n",
    "    varProof = -500 # 取最后的500个\n",
    "    proofImages = trainImages[varProof:0]\n",
    "    proofLabels = trainLabels[varProof:0]\n",
    "    trainImages = trainImages[0:varProof]\n",
    "    trainLabels = trainLabels[0:varProof]\n",
    "\n",
    "    # 对数据进行归一化，这里其实也可以做一个参数选项，可选可不选\n",
    "    # https://blog.csdn.net/sdgfbhgfj/article/details/123780347\n",
    "    if True:\n",
    "        mean = np.mean(trainImages,axis=0)\n",
    "    else:\n",
    "        mean = np.zeros_like(trainImages)\n",
    "    trainImages -= mean\n",
    "    proofImages -= mean\n",
    "    testImages -= mean\n",
    "    print('加载完毕')\n",
    "    # 依次返回训练数据&标签 验证数据&标签 测试数据&标签\n",
    "    return trainImages,trainLabels,proofImages,proofLabels,testImages,testLabels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "魔数:2051, 图片数量: 60000张, 图片大小: 28*28\n",
      "本次解析的矩阵格式为[60000,28,28]\n",
      "魔数:2049, 图片数量: 60000张, 图片大小: 1*1\n",
      "本次解析的矩阵格式为[60000,1,1]\n",
      "魔数:2051, 图片数量: 10000张, 图片大小: 28*28\n",
      "本次解析的矩阵格式为[10000,28,28]\n",
      "魔数:2049, 图片数量: 10000张, 图片大小: 1*1\n",
      "本次解析的矩阵格式为[10000,1,1]\n",
      "(60000, 32, 32)\n",
      "(60000, 1, 32, 32)\n",
      "加载完毕\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        ...,\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32),\n array([5, 0, 4, ..., 0, 1, 2]),\n array([], shape=(0, 1, 32, 32), dtype=float32),\n array([], dtype=int32),\n array([[[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        ...,\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]],\n \n \n        [[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32),\n array([7, 2, 1, ..., 4, 5, 6]))"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用来验证数据加载的正确性\n",
    "load_minist_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 在这个cell实现随机梯度下降\n",
    "\"\"\"\n",
    "随机梯度下降算法（Stochastic gradient descent，SGD）在神经网络模型训练中，是一种很常见的优化算法。\n",
    "这个算法的流程就是在每次更新的时候使用一个样本进行梯度下降，所谓的随机二字，就是说我们可以随机用一个样本来表示所有的样本，来调整超参数θ\n",
    "程序中的参数命名来源于下面的博客：\n",
    "https://blog.csdn.net/Oscar6280868/article/details/90641638\n",
    "\"\"\"\n",
    "def sgd(theta,xj,parameters):\n",
    "    if parameters is None:\n",
    "        # 创建学习率参数\n",
    "        parameters = {'learn_rate': 1e-2}\n",
    "    theta = theta - parameters['learn_rate'] * xj\n",
    "    return theta,parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 实现一个自适应动量的随机优化方法\n",
    "# 我觉得到时候看看能不能把这个删掉...太明显了\n",
    "\"\"\"\n",
    "论文：https://arxiv.org/pdf/1412.6980.pdf\n",
    "参数说明：\n",
    "t t：更新的步数（steps）\n",
    "α lphaα：学习率，用于控制步幅（stepsize）\n",
    "θ hetaθ：要求解（更新）的参数\n",
    "β1 一阶矩衰减系数\n",
    "β2 二阶矩衰减系数\n",
    "f（θ） 目标函数\n",
    "g 目标函数对θ求导所得梯度\n",
    "m 梯度g的一阶矩\n",
    "v 梯度g的二阶矩\n",
    "\n",
    "参考的博客：https://blog.csdn.net/sinat_36618660/article/details/100026261\n",
    "\"\"\"\n",
    "def adam(theta,xj,parameters):\n",
    "    if parameters is None:\n",
    "        parameters = {\n",
    "            \"learn_rate\":1e-2,\n",
    "            \"β1\":0.9,\n",
    "            \"β2\":0.9,\n",
    "            \"e\":1e-9,\n",
    "            \"m\":np.zeros_like(theta),\n",
    "            \"v\":np.zeros_like(theta),\n",
    "            \"t\":0\n",
    "        }\n",
    "    b1=parameters['β1']\n",
    "    b2=parameters['β2']\n",
    "    parameters['t'] = parameters['t'] + 1\n",
    "    parameters['m'] = ((1 - b1) * xj) + (b1 * parameters['m'])\n",
    "    parameters['v'] = ((1 - b2) * (xj ** 2)) + (b2 * parameters['v'])\n",
    "    temp1 = 1 - (b1 ** parameters['t'])\n",
    "    temp2 = 1 - (b2 ** parameters['t'])\n",
    "    new_theta = theta - parameters['learn_rate'] * (parameters['m'] / temp1) / (np.sqrt(parameters['v'] / temp2) + parameters['e'])\n",
    "    return new_theta,parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \"\"\"\n",
    "    Lenet5 相当于传进来一个类，这个类保存了一次训练需要的信息\n",
    "    data\n",
    "    epoch 所有样本都跑过一遍所需次数\n",
    "    func 选择哪个参数优化函数\n",
    "    parameter\n",
    "    decrease 学习率下降数\n",
    "    batchSize 每次只使用数据集中的部分样本\n",
    "    exp\n",
    "    \"\"\"\n",
    "    def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\n",
    "        self.model = Lenet5\n",
    "        self.train1 = data[\"train1\"] #在传进来data前也得注意\n",
    "        self.train2 = data[\"train2\"]\n",
    "        self.label1 = data[\"label1\"]\n",
    "        self.label2 = data[\"label2\"]\n",
    "        self.func = globals()[func] #让他指向了指定的那个函数\n",
    "        self.learn_rate_decay = decrease #学习率下降\n",
    "        self.batchSize = batchSize #一次使用的样本数\n",
    "        self.exp = exp #不知道干啥用的 完全没用到\n",
    "        self.epoch = epoch\n",
    "\n",
    "        # 保存最高准确率以及最佳参数\n",
    "        self.bestAccurate = 0\n",
    "        self.bestPara = {}\n",
    "\n",
    "        # 为了绘图\n",
    "        self.losses = []\n",
    "        self.TrainAccurates = []\n",
    "        self.ProofAccurates = []\n",
    "\n",
    "        # 按照他的意思这个东西是深度拷贝\n",
    "        self.funcParameter = {}\n",
    "        for i in self.moodel.params:\n",
    "            d = {k: v for k,v in parameter.items()}\n",
    "            self.funcParameter[i] = d\n",
    "\n",
    "    # 直接返回向量\n",
    "    def calAccuracy(self,train,label,batchSize):\n",
    "        # 需要做几轮运算\n",
    "        myRound = math.ceil(train.shape[0] / batchSize)\n",
    "        # 最终的预测结果\n",
    "        prediction = []\n",
    "        for i in range(myRound):\n",
    "            start = i*batchSize\n",
    "            end = (i+1)*batchSize\n",
    "            # 这个不太懂loss算了个什么\n",
    "            # 为什么只用给train就行了，还有就是train是什么，最后数组保存的是什么\n",
    "            # 实际loss返回两个值\n",
    "            acc = self.model.loss(train[start:end]) #计算损失\n",
    "            p = np.argmax(acc,axis=1) #找出最大值\n",
    "            prediction.append(p)\n",
    "        np.hstack(prediction) #横向拼接向量\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def calAccuracy2(self,train,label,batchSize):\n",
    "        prediction = self.calAccuracy(train, label, batchSize)\n",
    "        return np.mean(prediction == label)\n",
    "\n",
    "    def myTrain(self):\n",
    "        # 为了绘图 把这几个先初始化为空\n",
    "        self.losses = []\n",
    "        self.TrainAccurates = []\n",
    "        self.ProofAccurates = []\n",
    "        # 按照不同的轮以对应的batchsize训练\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(math.ceil(self.train1.shape[0] / self.batchSize)):\n",
    "                start = j * self.batchSize\n",
    "                end = (j+1) * self.batchSize\n",
    "                arr = np.arange(start,end)\n",
    "                batch_train = self.train1[arr]\n",
    "                batch_label = self.label1[arr]\n",
    "                # 我觉得在算loss的时候就更新了model的params\n",
    "                loss,res = self.model.loss(batch_train,batch_label)\n",
    "                self.losses.append(loss)\n",
    "                for k,v in self.model.params.items():\n",
    "                    self.model.params[k],self.funcParameter[k] = self.func(v,res[k],self.funcParameter[k])\n",
    "                print('第 %d 轮第 %d 个batch中loss为 %f' % (i+1,j+1,loss))\n",
    "\n",
    "            self.TrainAccurates.append(self.calAccuracy2(self.train1,self.train2,100))\n",
    "            self.ProofAccurates.append(self.calAccuracy2(self.label1,self.label2,100))\n",
    "\n",
    "            if self.calAccuracy(self.label1,self.label2,100) > self.bestAccurate :\n",
    "                self.bestAccurate = self.calAccuracy(self.label1,self.label2,100)\n",
    "                self.bestPara.clear() #更新自己参数里的最佳参数\n",
    "                for k,v in self.model.params.items():\n",
    "                    self.bestPara[k] = v.copy()\n",
    "\n",
    "            if i+1 == self.epoch:\n",
    "                filename = \"TrainingResult\"\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump(self.model,f)\n",
    "                    print(\"成功保存模型\")\n",
    "\n",
    "            if i > 10:\n",
    "                for q in self.funcParameter:\n",
    "                    self.funcParameter[q]['learn_rate'] *= self.learn_rate_decay\n",
    "\n",
    "        # 我觉得这里最后一轮出最佳的话就保存不了了啊\n",
    "        self.model.params = self.bestPara"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 卷积层前向传播的过程\n",
    "def convolution_spread_forward(train, convolution_entity, bias, para):\n",
    "    pic_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train[2]\n",
    "    wide = train[3]\n",
    "    convolution_num = convolution_entity[0]\n",
    "    convolution_height = convolution_entity[2]\n",
    "    convolution_wide = convolution_entity[3]\n",
    "    step = para['step'] #这里代表着步长\n",
    "    padding = para['pad'] # 填充部分的大小\n",
    "    # 这里可以改成函数外提\n",
    "    new_train = np.pad(train,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    # 这里也可以函数外提\n",
    "    loop_outside  = height + 2 * padding - convolution_height\n",
    "    loop_outside  = loop_outside / step\n",
    "    loop_outside  = math.floor(loop_outside) + 1\n",
    "    loop_inside = wide + 2 * padding - convolution_wide\n",
    "    loop_inside = loop_inside / step\n",
    "    loop_inside = math.floor(loop_inside) + 1\n",
    "    weight = np.zeros(shape=((pic_num,convolution_num,loop_outside,loop_inside)),dtype=float)\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = []\n",
    "            # 这个矩阵转换必须外提做函数\n",
    "            temp = new_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide]\n",
    "            temp.reshape(pic_num,1,channel,convolution_height,convolution_wide)\n",
    "            new_convolution = convolution_entity.reshape(1,convolution_num,channel,convolution_height,convolution_wide)\n",
    "            # 这个也可以进行函数外提\n",
    "            weight = np.sum(temp * new_convolution,axis=(-3,-2,-1))\n",
    "            weight += bias\n",
    "    # 这个也得封装\n",
    "    package = (train,convolution_entity,bias,para)\n",
    "    return weight,package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convolution_spread_backward(derivation,pack):\n",
    "    num = derivation[0]\n",
    "    convolution_num = derivation[1]\n",
    "    loop_outside = derivation[2]\n",
    "    loop_inside = derivation[3]\n",
    "    train = pack[0]\n",
    "    height = train[2]\n",
    "    wide = train[3]\n",
    "    convolution = pack[1]\n",
    "    derivation_convolution = np.zeros_like(convolution)\n",
    "    channel =convolution[1]\n",
    "    convolution_height = convolution[2]\n",
    "    convolution_wide = convolution[3]\n",
    "    bias = pack[2]\n",
    "    derivation_bias = np.zeros_like(bias)\n",
    "    para = pack[3]\n",
    "    padding = para['pad']\n",
    "    step = para['step']\n",
    "    # 这个可以封装成一个比较复杂的函数\n",
    "    new_train = np.pad(train,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    derivation_train = np.zeros_like(new_train)\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            #都可以外提成函数\n",
    "            temp = derivation[:,:,i,j]\n",
    "            temp.reshape((num,1,1,1,convolution_num))\n",
    "            temp2 = convolution.transpose((1,2,3,0))\n",
    "            temp2.reshape((1,channel,convolution_height,convolution_wide,convolution_num))\n",
    "            temp_sum = np.sum(temp*temp2,axis=-1)\n",
    "            derivation_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide] +=temp_sum\n",
    "            temp3 = derivation[:,:,i,j].T\n",
    "            temp3 = temp3.reshape((convolution_num,1,1,1,num))\n",
    "            temp4 = new_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide]\n",
    "            # 外提做函数\n",
    "            temp4.transpose(1,2,3,0)\n",
    "            temp_sum = np.sum(temp3*temp4,axis=-1)\n",
    "            derivation_convolution += temp_sum\n",
    "            temp_sum = np.sum(derivation[:,:,i,j],axis=0)\n",
    "            derivation_bias += temp_sum\n",
    "\n",
    "    # 这个必须外提\n",
    "    derivation_weight = []\n",
    "    derivation_weight = derivation_train[:,:,padding:padding+height,padding:padding+wide]\n",
    "    return derivation_weight,derivation_convolution,derivation_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
