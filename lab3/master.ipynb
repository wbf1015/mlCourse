{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "outputs": [],
   "source": [
    "# 加载minist数据集\n",
    "# 加载数据集这方面的工作参考了前人的工作，网页为：https://blog.csdn.net/hxxjxw/article/details/113727973\n",
    "def load_mnist(mnistdir , train):\n",
    "    ministfile = open(mnistdir,'rb')\n",
    "    ministdata = ministfile.read()\n",
    "    ministfile.close()\n",
    "    rows=1\n",
    "    cols=1\n",
    "    # 加载训练集\n",
    "    if train:\n",
    "        # 解析文件头信息，依次为魔数、图片数量、每张图片高、每张图片宽\n",
    "        # 因为数据结构中前4行的数据类型都是32位整型，所以采用i格式，但我们需要读取前4行数据，所以需要4个i。我们后面会看到标签集中，只使用2个ii\n",
    "        magic_num,images,rows,cols = struct.unpack_from('>iiii', ministdata,0)\n",
    "    else:\n",
    "        # 加载标签集\n",
    "        magic_num,images = struct.unpack_from('>ii', ministdata,0)\n",
    "    print('魔数:%d, 图片数量: %d张, 图片大小: %d*%d' % (magic_num, images, rows, cols))\n",
    "    # 计算加载的总像素是多少\n",
    "    size = images * rows * cols\n",
    "    # calcsize获得数据在缓存中的指针位置，从前面介绍的数据结构可以看出，读取了前4行之后，指针位置（即偏移位置offset）指向0016\n",
    "    if train:\n",
    "        pointer = struct.calcsize('>iiii')\n",
    "    else :\n",
    "        pointer =  struct.calcsize('>ii')\n",
    "    pack_data = struct.unpack_from('>' + str(size) + 'B', ministdata,pointer)\n",
    "    if train:\n",
    "        pack_data = np.reshape(pack_data,[images,rows,cols])\n",
    "    else:\n",
    "        pack_data = np.reshape(pack_data,[images])\n",
    "    # 最终返回了一个矩阵，矩阵的大小由是训练集还是标签集决定\n",
    "    # 训练集就相当于返回好多页纸，每一页纸上面有对应的行数和列数\n",
    "    print('本次解析的矩阵格式为[%d,%d,%d]' % (images,rows,cols))\n",
    "    return pack_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "outputs": [],
   "source": [
    "# 在本函数中完成了本地的数据加载\n",
    "# 根据参数取出相应的验证集并做正则化\n",
    "def load_minist_data():\n",
    "    trainImages = load_mnist(\"data/train-images.idx3-ubyte\",True)\n",
    "    trainLabels = load_mnist(\"data/train-labels.idx1-ubyte\",False)\n",
    "    testImages = load_mnist(\"data/t10k-images.idx3-ubyte\",True)\n",
    "    testLabels = load_mnist(\"data/t10k-labels.idx1-ubyte\",False)\n",
    "    # 其实这里不知道为什么要pad，但还是先pad一下\n",
    "    # 我觉得可能是因为卷积的时候不丢失信息吧\n",
    "    # 对矩阵进行进行填充\n",
    "    # https://blog.csdn.net/qq_34650787/article/details/80500407\n",
    "    trainImages = np.pad(trainImages, ((0, 0), (2, 2), (2, 2)))\n",
    "    testImages = np.pad(testImages, ((0, 0), (2, 2), (2, 2)))\n",
    "\n",
    "    # 获取矩阵维度\n",
    "    real_num, real_rows, real_cols = trainImages.shape\n",
    "    real_num2,real_rows2,real_cols2 = testImages.shape\n",
    "    # print('real_rows=%d,real_cols=%d' % (real_rows,real_cols))\n",
    "\n",
    "    # 这个类型转换我也搞不太懂\n",
    "    # 而且也不知道这个reshape的目的在哪里\n",
    "    # 进行类型转换 https://blog.csdn.net/u012267725/article/details/77489244\n",
    "    # 我的理解是在这里把很多页书拼起来拼成一页，但是这一页很长\n",
    "    # print(trainImages.shape) #:这里他的输出是（60000,32,32）\n",
    "    trainImages = trainImages.astype(np.float32).reshape(real_num, 1, real_rows, real_cols)\n",
    "    testImages = testImages.astype(np.float32).reshape(real_num2, 1, real_rows, real_cols)\n",
    "    # print(trainImages.shape) #:在这里输出就变成了（60000,1,32,32）\n",
    "\n",
    "    # 接下来在训练集中划分验证集\n",
    "    # 这里的参数是可以调整的\n",
    "    varProof = -500 # 取最后的500个\n",
    "    proofImages = trainImages[varProof:]\n",
    "    proofLabels = trainLabels[varProof:]\n",
    "    trainImages = trainImages[:varProof]\n",
    "    trainLabels = trainLabels[:varProof]\n",
    "\n",
    "    # 对数据进行归一化，这里其实也可以做一个参数选项，可选可不选\n",
    "    # https://blog.csdn.net/sdgfbhgfj/article/details/123780347\n",
    "    if True:\n",
    "        mean = np.mean(trainImages,axis=0)\n",
    "    else:\n",
    "        mean = np.zeros_like(trainImages)\n",
    "    trainImages -= mean\n",
    "    proofImages -= mean\n",
    "    testImages -= mean\n",
    "    print('加载完毕')\n",
    "    # 依次返回训练数据&标签 验证数据&标签 测试数据&标签\n",
    "    return trainImages,trainLabels,proofImages,proofLabels,testImages,testLabels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "outputs": [],
   "source": [
    "# 在这个cell实现随机梯度下降\n",
    "\"\"\"\n",
    "随机梯度下降算法（Stochastic gradient descent，SGD）在神经网络模型训练中，是一种很常见的优化算法。\n",
    "这个算法的流程就是在每次更新的时候使用一个样本进行梯度下降，所谓的随机二字，就是说我们可以随机用一个样本来表示所有的样本，来调整超参数θ\n",
    "程序中的参数命名来源于下面的博客：\n",
    "https://blog.csdn.net/Oscar6280868/article/details/90641638\n",
    "\"\"\"\n",
    "def sgd(theta,xj,parameters):\n",
    "    if parameters is None:\n",
    "        # 创建学习率参数\n",
    "        parameters = {'learn_rate': 1e-2}\n",
    "    theta = theta - parameters['learn_rate'] * xj\n",
    "    return theta,parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "outputs": [],
   "source": [
    "# 实现一个自适应动量的随机优化方法\n",
    "# 我觉得到时候看看能不能把这个删掉...太明显了\n",
    "\"\"\"\n",
    "论文：https://arxiv.org/pdf/1412.6980.pdf\n",
    "参数说明：\n",
    "t t：更新的步数（steps）\n",
    "α lphaα：学习率，用于控制步幅（stepsize）\n",
    "θ hetaθ：要求解（更新）的参数\n",
    "β1 一阶矩衰减系数\n",
    "β2 二阶矩衰减系数\n",
    "f（θ） 目标函数\n",
    "g 目标函数对θ求导所得梯度\n",
    "m 梯度g的一阶矩\n",
    "v 梯度g的二阶矩\n",
    "\n",
    "参考的博客：https://blog.csdn.net/sinat_36618660/article/details/100026261\n",
    "\"\"\"\n",
    "def adam(theta,xj,parameters):\n",
    "    if parameters is None:\n",
    "        parameters={}\n",
    "    parameters.setdefault(\"learn_rate\", 1e-3)\n",
    "    parameters.setdefault(\"beta1\", 0.9)\n",
    "    parameters.setdefault(\"beta2\", 0.999)\n",
    "    parameters.setdefault(\"e\", 1e-8)\n",
    "    parameters.setdefault(\"m\", np.zeros_like(theta))\n",
    "    parameters.setdefault(\"v\", np.zeros_like(theta))\n",
    "    parameters.setdefault(\"t\", 0)\n",
    "\n",
    "\n",
    "    b1=parameters['beta1']\n",
    "    b2=parameters['beta2']\n",
    "    parameters['t'] = parameters['t'] + 1\n",
    "    parameters['m'] = ((1 - b1) * xj) + (b1 * parameters['m'])\n",
    "    parameters['v'] = ((1 - b2) * (xj ** 2)) + (b2 * parameters['v'])\n",
    "    temp1 = 1 - (b1 ** parameters['t'])\n",
    "    temp2 = 1 - (b2 ** parameters['t'])\n",
    "    new_theta = theta - parameters['learn_rate'] * (parameters['m'] / temp1) / (np.sqrt(parameters['v'] / temp2) + parameters['e'])\n",
    "    return new_theta,parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "outputs": [],
   "source": [
    "\n",
    "class Train:\n",
    "    \"\"\"\n",
    "    Lenet5 相当于传进来一个类，这个类保存了一次训练需要的信息\n",
    "    data\n",
    "    epoch 所有样本都跑过一遍所需次数\n",
    "    func 选择哪个参数优化函数\n",
    "    parameter\n",
    "    decrease 学习率下降数\n",
    "    batchSize 每次只使用数据集中的部分样本\n",
    "    exp\n",
    "    \"\"\"\n",
    "    # 这里其实可以初始化一下\n",
    "    def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\n",
    "        self.model = Lenet5\n",
    "        self.train1 = data[\"train1\"] #在传进来data前也得注意\n",
    "        self.train2 = data[\"train2\"]\n",
    "        self.label1 = data[\"label1\"]\n",
    "        self.label2 = data[\"label2\"]\n",
    "        self.func = func #让他指向了指定的那个函数\n",
    "        self.learn_rate_decay = decrease #学习率下降\n",
    "        self.batchSize = batchSize #一次使用的样本数\n",
    "        self.exp = exp #不知道干啥用的 完全没用到\n",
    "        self.epoch = epoch\n",
    "\n",
    "        # 保存最高准确率以及最佳参数\n",
    "        self.bestAccurate = 0\n",
    "        self.bestPara = {}\n",
    "\n",
    "        # 为了绘图\n",
    "        self.losses = []\n",
    "        self.TrainAccurates = []\n",
    "        self.ProofAccurates = []\n",
    "\n",
    "        # 按照他的意思这个东西是深度拷贝\n",
    "        self.funcParameter = {}\n",
    "        for i in self.model.para:\n",
    "            d = {k: v for k,v in parameter.items()}\n",
    "            self.funcParameter[i] = d\n",
    "\n",
    "    # 直接返回向量\n",
    "    def calAccuracy(self,train,label,batchSize):\n",
    "        # 需要做几轮运算\n",
    "        myRound = math.ceil(train.shape[0] / batchSize)\n",
    "        # 最终的预测结果\n",
    "        prediction = []\n",
    "        for i in range(myRound):\n",
    "            start = i*batchSize\n",
    "            end = (i+1)*batchSize\n",
    "            # 这个不太懂loss算了个什么\n",
    "            # 为什么只用给train就行了，还有就是train是什么，最后数组保存的是什么\n",
    "            # 实际loss返回两个值\n",
    "            acc = self.model.spread2(train[start:end]) #计算损失\n",
    "            p = np.argmax(acc,axis=1) #找出最大值\n",
    "            prediction.append(p)\n",
    "        prediction = np.hstack(prediction) #横向拼接向量\n",
    "        return prediction\n",
    "\n",
    "    def calAccuracy2(self,train,label,batchSize):\n",
    "        prediction = self.calAccuracy(train, label, batchSize)\n",
    "        return np.mean(prediction == label)\n",
    "\n",
    "    def myTrain(self):\n",
    "        # 为了绘图 把这几个先初始化为空\n",
    "        self.losses = []\n",
    "        self.TrainAccurates = []\n",
    "        self.ProofAccurates = []\n",
    "        # 按照不同的轮以对应的batchsize训练\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(math.ceil(self.train1.shape[0] / self.batchSize)-1):\n",
    "                start = j * self.batchSize\n",
    "                end = (j+1) * self.batchSize\n",
    "                arr = np.arange(start,end)\n",
    "                batch_train = self.train1[arr]\n",
    "                batch_label = self.label1[arr]\n",
    "                # 我觉得在算loss的时候就更新了model的params\n",
    "                loss,res = self.model.spread(batch_train,batch_label)\n",
    "                self.losses.append(loss)\n",
    "                for k,v in self.model.para.items():\n",
    "                    if self.func =='sgd':\n",
    "                        self.model.para[k],self.funcParameter[k] = sgd(v,res[k],self.funcParameter[k])\n",
    "                    else:\n",
    "                        self.model.para[k],self.funcParameter[k] = adam(v,res[k],self.funcParameter[k])\n",
    "                print('第 %d 轮第 %d 个batch中loss为 %f' % (i+1,j+1,loss))\n",
    "\n",
    "            a = self.calAccuracy2(self.train1,self.label1,100)\n",
    "            b = self.calAccuracy2(self.train2,self.label2,100)\n",
    "            self.TrainAccurates.append(a)\n",
    "            self.ProofAccurates.append(b)\n",
    "\n",
    "            print('第 %d 次迭代，训练集正确率 %f 验证集正确率 %f' % (i+1,a,b))\n",
    "\n",
    "            if b > self.bestAccurate :\n",
    "                self.bestAccurate = b\n",
    "                self.bestPara.clear() #更新自己参数里的最佳参数\n",
    "                for k,v in self.model.para.items():\n",
    "                    self.bestPara[k] = v.copy()\n",
    "\n",
    "            if i+1 == self.epoch:\n",
    "                filename = \"TrainingResult\"\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump(self.model,f)\n",
    "                    print(\"成功保存模型\")\n",
    "\n",
    "            if i > 10:\n",
    "                for q in self.funcParameter:\n",
    "                    self.funcParameter[q]['learn_rate'] *= self.learn_rate_decay\n",
    "\n",
    "        # 我觉得这里最后一轮出最佳的话就保存不了了啊\n",
    "        self.model.params = self.bestPara"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "outputs": [],
   "source": [
    "# 卷积层前向传播的过程\n",
    "def convolution_spread_forward(train, convolution_entity, bias, para):\n",
    "    pic_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    convolution_num = convolution_entity.shape[0]\n",
    "    convolution_height = convolution_entity.shape[2]\n",
    "    convolution_wide = convolution_entity.shape[3]\n",
    "    step = para['step'] #这里代表着步长\n",
    "    padding = para['pad'] # 填充部分的大小\n",
    "    # 这里可以改成函数外提\n",
    "    new_train = np.pad(train,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    # 这里也可以函数外提\n",
    "    loop_outside  = height + 2 * padding - convolution_height\n",
    "    loop_outside  = loop_outside // step\n",
    "    loop_outside  = loop_outside + 1\n",
    "    loop_inside = wide + 2 * padding - convolution_wide\n",
    "    loop_inside = loop_inside // step\n",
    "    loop_inside = loop_inside + 1\n",
    "    weight = np.zeros((pic_num, convolution_num, loop_outside, loop_inside))\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = []\n",
    "            # 这个矩阵转换必须外提做函数\n",
    "            temp = new_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide]\n",
    "            temp = temp.reshape(pic_num,1,channel,convolution_height,convolution_wide)\n",
    "            new_convolution = convolution_entity.reshape(1,convolution_num,channel,convolution_height,convolution_wide)\n",
    "            # 这个也可以进行函数外提\n",
    "            weight[:,:,i,j] = np.sum(temp * new_convolution,axis=(-3,-2,-1))\n",
    "            weight[:,:,i,j] += bias\n",
    "    # 这个也得封装\n",
    "    package = (train,convolution_entity,bias,para)\n",
    "    return weight,package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "outputs": [],
   "source": [
    "def convolution_spread_backward(derivation,pack):\n",
    "    num = derivation.shape[0]\n",
    "    convolution_num = derivation.shape[1]\n",
    "    loop_outside = derivation.shape[2]\n",
    "    loop_inside = derivation.shape[3]\n",
    "    train = pack[0]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    convolution = pack[1]\n",
    "    derivation_convolution = np.zeros_like(convolution)\n",
    "    channel =convolution.shape[1]\n",
    "    convolution_height = convolution.shape[2]\n",
    "    convolution_wide = convolution.shape[3]\n",
    "    bias = pack[2]\n",
    "    derivation_bias = np.zeros_like(bias)\n",
    "    para = pack[3]\n",
    "    padding = para['pad']\n",
    "    step = para['step']\n",
    "    # 这个可以封装成一个比较复杂的函数\n",
    "    new_train = np.pad(train,((0,0),(0,0),(padding,padding),(padding,padding)))\n",
    "    derivation_train = np.zeros_like(new_train)\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            #都可以外提成函数\n",
    "            temp = derivation[:,:,i,j]\n",
    "            temp = temp.reshape((num,1,1,1,convolution_num))\n",
    "            temp2 = convolution.transpose((1,2,3,0))\n",
    "            temp2 = temp2.reshape((1,channel,convolution_height,convolution_wide,convolution_num))\n",
    "            temp_sum = np.sum(temp*temp2,axis=-1)\n",
    "            derivation_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide] +=temp_sum\n",
    "            temp3 = derivation[:,:,i,j].T\n",
    "            temp3 = temp3.reshape((convolution_num,1,1,1,num))\n",
    "            temp4 = new_train[:,:,step*i:step*i+convolution_height,step*j:step*j+convolution_wide].transpose(1,2,3,0)\n",
    "            # 外提做函数\n",
    "            temp_sum = np.sum(temp3*temp4,axis=-1)\n",
    "            derivation_convolution += temp_sum\n",
    "            temp_sum = np.sum(derivation[:,:,i,j],axis=0)\n",
    "            derivation_bias += temp_sum\n",
    "\n",
    "    # 这个必须外提\n",
    "    derivation_weight = []\n",
    "    derivation_weight = derivation_train[:,:,padding:padding+height,padding:padding+wide]\n",
    "    return derivation_weight,derivation_convolution,derivation_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "outputs": [],
   "source": [
    "def relu_spread_froward(Input):\n",
    "    # 由于老师在课堂上强调sigmoid的梯度消失问题\n",
    "    # 解决办法可以为替换激活函数\n",
    "    # 使用relu作为激活函数\n",
    "    temp = np.maximum(0,Input)\n",
    "    return temp,Input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "outputs": [],
   "source": [
    "def relu_spread_backward(derivation,Input):\n",
    "    # relu函数的反向传播函数\n",
    "    temp = Input\n",
    "    temp[temp>0]=1\n",
    "    temp[temp<=0]=0\n",
    "    derivation_train = derivation * Input\n",
    "    return derivation_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "outputs": [],
   "source": [
    "def pool_spread_forward(train,para):\n",
    "    pool_height = para['pool_height'] #传入的时候也要按照这个格式命名\n",
    "    pool_wide = para['pool_wide']\n",
    "    step = para['step']\n",
    "    train_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    loop_outside = height - pool_height\n",
    "    loop_outside = loop_outside // step\n",
    "    loop_outside = loop_outside + 1\n",
    "    loop_inside = wide - pool_wide\n",
    "    loop_inside = loop_inside // step\n",
    "    loop_inside = loop_inside + 1\n",
    "    ret = np.zeros(shape=(train_num,channel,loop_outside,loop_inside))\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = train[:,:,i*step:i*step+pool_height,j*step:j*step+pool_wide]\n",
    "            ret[:,:,i,j]=np.max(temp,axis=(2,3))\n",
    "    pack = (train,para)\n",
    "    return ret,pack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "outputs": [],
   "source": [
    "def pool_spread_backward(derivation,pack):\n",
    "    train = pack[0]\n",
    "    derivation_train = np.zeros_like(train)\n",
    "    train_num = train.shape[0]\n",
    "    channel = train.shape[1]\n",
    "    height = train.shape[2]\n",
    "    wide = train.shape[3]\n",
    "    para = pack[1]\n",
    "    pool_height = para['pool_height']\n",
    "    pool_wide = para['pool_wide']\n",
    "    step = para['step']\n",
    "    loop_outside = height - pool_height\n",
    "    loop_outside = loop_outside // step\n",
    "    loop_outside = loop_outside + 1\n",
    "    loop_inside = wide - pool_wide\n",
    "    loop_inside = loop_inside // step\n",
    "    loop_inside = loop_inside + 1\n",
    "    for i in range(loop_outside):\n",
    "        for j in range(loop_inside):\n",
    "            temp = train[:,:,i*step:i*step+pool_height,j*step:j*step+pool_wide]\n",
    "            temp = temp.reshape(train_num,channel,-1)\n",
    "            tempMax = np.argmax(temp,axis=-1)\n",
    "            # 这个必须外提\n",
    "            temp_max = np.unravel_index(tempMax,(pool_height,pool_wide))\n",
    "            which = np.array(temp_max)\n",
    "            for k in range(train_num):\n",
    "                for l in range(channel):\n",
    "                    # 这个都可以外提其实\n",
    "                    temp2 = derivation[k][l][i][j]\n",
    "                    index_3 = i*step+which[0][k][l]\n",
    "                    index_4 = j*step+which[1][k][l]\n",
    "                    derivation_train[k][l][index_3][index_4] += temp2\n",
    "\n",
    "    return derivation_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "outputs": [],
   "source": [
    "def full_connect_spread_forward(train,weight,bias):\n",
    "    # 全连接层的前向传播\n",
    "    index_0 = train.shape[0]\n",
    "    temp = train.reshape(index_0,-1)\n",
    "    temp = temp.dot(weight) + bias\n",
    "    pack = (train,weight,bias)\n",
    "    return temp,pack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "outputs": [],
   "source": [
    "def full_connect_spread_backward(derivation,pack):\n",
    "    train = pack[0]\n",
    "    index_0 = train.shape[0]\n",
    "    weight = pack[1]\n",
    "    weight_T = weight.T\n",
    "    bias = pack[2]\n",
    "    derivation_train = derivation.dot(weight_T)\n",
    "    derivation_train = derivation_train.reshape(train.shape)\n",
    "    temp = train.reshape(index_0,-1)\n",
    "    temp_T = temp.T\n",
    "    derivation_weight = temp_T.dot(derivation)\n",
    "    derivation_bias = np.sum(derivation,axis=0)\n",
    "    return derivation_train,derivation_weight,derivation_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "outputs": [],
   "source": [
    "def softmax(Input,Output):\n",
    "    # softmax损失函数的计算\n",
    "    data_num = Input.shape[0]\n",
    "    cal_e = np.exp(Input)\n",
    "    cal_sum = np.sum(cal_e,axis=1)\n",
    "    cal_log = np.log(cal_sum)\n",
    "    temp = Input[range(data_num),list(Output)]\n",
    "    ret = np.mean(cal_log-temp)\n",
    "    temp_sum = cal_sum.reshape(data_num,1)\n",
    "    derivation_Input = cal_e / temp_sum\n",
    "    # 必须外提\n",
    "    derivation_Input[range(data_num),list(Output)] = derivation_Input[range(data_num),list(Output)]-1\n",
    "    derivation_Input = derivation_Input / data_num\n",
    "    return ret,derivation_Input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "outputs": [],
   "source": [
    "class MyLenet5:\n",
    "    def __init__(self):\n",
    "        # 输入的图像通道为1\n",
    "        # 输入图像大小为32*32\n",
    "        self.input_channel = 1\n",
    "        self.input_height = 32\n",
    "        self.input_wide = 32\n",
    "        # 第一次卷积核有6个\n",
    "        # 第二次卷积核有16个\n",
    "        # 卷积核大小都是5*5\n",
    "        self.convolution_num1 = 6\n",
    "        self.convolution_num2 = 16\n",
    "        self.convolution_height = 5\n",
    "        self.convolution_wide =5\n",
    "        # 全连接层的大小\n",
    "        self.full_connect1 = 400\n",
    "        self.full_connect2 = 120\n",
    "        self.full_connect3 = 84\n",
    "        # 输出类型个数\n",
    "        self.output_size = 10\n",
    "        # lenet5的参数\n",
    "        # 卷积池化层的参数和偏置项\n",
    "        # 全连接层的参数和偏置项\n",
    "        self.para = {}\n",
    "        # 正则项\n",
    "        self.regularItem = 0.001\n",
    "        self.datatype = np.float32\n",
    "        # 生成时必要的标准差\n",
    "        self.standard_deviation = 0.001\n",
    "\n",
    "        #首先初始化第一个卷积层\n",
    "        convolution1_para_weight = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale=self.standard_deviation,\n",
    "            size=(self.convolution_num1,self.input_channel,self.convolution_height,self.convolution_wide)\n",
    "        )\n",
    "        convolution1_para_bias = np.zeros(self.convolution_num1)\n",
    "\n",
    "        #接下来初始化第二个卷积层\n",
    "        convolution2_para_weight = np.random.normal(\n",
    "            loc = 0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.convolution_num2,self.convolution_num1,self.convolution_height,self.convolution_wide)\n",
    "        )\n",
    "        convolution2_para_bias = np.zeros(self.convolution_num2)\n",
    "\n",
    "        #接下来初始化第一个全连接层\n",
    "        full_connect1_para_weight = np.random.normal(\n",
    "            loc = 0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.full_connect1,self.full_connect2)\n",
    "        )\n",
    "        full_connect1_para_bias = np.zeros(self.full_connect2)\n",
    "\n",
    "        #接下来初始化第二个全连接层\n",
    "        full_connect2_para_weight = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.full_connect2,self.full_connect3)\n",
    "        )\n",
    "        full_connect2_para_bias = np.zeros(self.full_connect3)\n",
    "\n",
    "        #接下来初始化第三个全连接层\n",
    "        full_connect3_para_weight = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale = self.standard_deviation,\n",
    "            size = (self.full_connect3,self.output_size)\n",
    "        )\n",
    "        full_connect3_para_bias = np.zeros(self.output_size)\n",
    "\n",
    "        self.para['convolution1_para_weight'] = convolution1_para_weight\n",
    "        self.para['convolution1_para_bias'] = convolution1_para_bias\n",
    "        self.para['convolution2_para_weight'] = convolution2_para_weight\n",
    "        self.para['convolution2_para_bias'] = convolution2_para_bias\n",
    "        self.para['full_connect1_para_weight'] = full_connect1_para_weight\n",
    "        self.para['full_connect1_para_bias'] = full_connect1_para_bias\n",
    "        self.para['full_connect2_para_weight'] = full_connect2_para_weight\n",
    "        self.para['full_connect2_para_bias'] = full_connect2_para_bias\n",
    "        self.para['full_connect3_para_weight'] = full_connect3_para_weight\n",
    "        self.para['full_connect3_para_bias'] = full_connect3_para_bias\n",
    "\n",
    "        self.changeType()\n",
    "\n",
    "    def changeType(self):\n",
    "        for k,v in self.para.items():\n",
    "            self.para[k] = v.astype(np.float32)\n",
    "\n",
    "    # 这个在不同的层那里要改\n",
    "    def spread(self,train,label):\n",
    "        # 卷积层步长为1，没有padding\n",
    "        convolution_para = {}\n",
    "        convolution_para.clear()\n",
    "        convolution_para['step']=1\n",
    "        convolution_para['pad']=0\n",
    "\n",
    "        # 池化层步长为2 范围就是【2,2】\n",
    "        pooling_para={}\n",
    "        pooling_para.clear()\n",
    "        pooling_para['step']=2\n",
    "        pooling_para['pool_height']=2\n",
    "        pooling_para['pool_wide']=2\n",
    "\n",
    "        # 拿出所有需要的权重\n",
    "        c1w = self.para['convolution1_para_weight']\n",
    "        temp1 = np.sum(c1w*c1w)\n",
    "        c1b = self.para['convolution1_para_bias']\n",
    "        c2w = self.para['convolution2_para_weight']\n",
    "        temp2 = np.sum(c2w*c2w)\n",
    "        c2b = self.para['convolution2_para_bias']\n",
    "        fc3w = self.para['full_connect1_para_weight']\n",
    "        temp3 = np.sum(fc3w*fc3w)\n",
    "        fc3b = self.para['full_connect1_para_bias']\n",
    "        fc4w = self.para['full_connect2_para_weight']\n",
    "        temp4 = np.sum(fc4w*fc4w)\n",
    "        fc4b = self.para['full_connect2_para_bias']\n",
    "        fc5w = self.para['full_connect3_para_weight']\n",
    "        temp5 = np.sum(fc5w*fc5w)\n",
    "        fc5b = self.para['full_connect3_para_bias']\n",
    "\n",
    "        cal_temp = temp1 + temp2 +temp3 +temp4 +temp5\n",
    "        cal_temp = (self.regularItem*cal_temp)/2\n",
    "        #开始前向传播\n",
    "\n",
    "        # 第一次卷积层+relu+第一层池化层\n",
    "        c1_res,c1_pack = convolution_spread_forward(train,c1w,c1b,convolution_para)\n",
    "\n",
    "        c1_relu_res,c1_relu_pack = relu_spread_froward(c1_res)\n",
    "\n",
    "        c1_pool_res,c1_pool_pack = pool_spread_forward(c1_relu_res,pooling_para)\n",
    "\n",
    "        # 第二次卷积层+relu+第二层池化层\n",
    "        c2_res,c2_pack = convolution_spread_forward(c1_pool_res,c2w,c2b,convolution_para)\n",
    "\n",
    "        c2_relu_res,c2_relu_pack = relu_spread_froward(c2_res)\n",
    "\n",
    "        c2_pool_res,c2_pool_pack = pool_spread_forward(c2_relu_res,pooling_para)\n",
    "\n",
    "        # 第一层全连接层+relu层\n",
    "        c3_res,c3_pack = full_connect_spread_forward(c2_pool_res,fc3w,fc3b)\n",
    "\n",
    "        c3_relu_res,c3_relu_pack = relu_spread_froward(c3_res)\n",
    "\n",
    "        # 第二层全连接层\n",
    "        c4_res,c4_pack = full_connect_spread_forward(c3_relu_res,fc4w,fc4b)\n",
    "\n",
    "        c4_relu_res,c4_relu_pack = relu_spread_froward(c4_res)\n",
    "\n",
    "        # 输出层\n",
    "        c5_out,c5_pack = full_connect_spread_forward(c4_relu_res,fc5w,fc5b)\n",
    "\n",
    "        # 开始反向传播\n",
    "\n",
    "        # 声明校正值为0.0\n",
    "        correction1 = 0.0\n",
    "        correction2 = 0.0\n",
    "        correction3 = 0.0\n",
    "        correction4 = 0.0\n",
    "        correction5 = 0.0\n",
    "        correction1_2 = 0.0\n",
    "        correction2_2 = 0.0\n",
    "        correction3_2 = 0.0\n",
    "        correction4_2 = 0.0\n",
    "        correction5_2 = 0.0\n",
    "\n",
    "\n",
    "        l,derivation_train = softmax(c5_out,label)\n",
    "\n",
    "        l = l + cal_temp\n",
    "\n",
    "        derivation_train,correction5,correction5_2 = full_connect_spread_backward(derivation_train,c5_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c4_relu_pack)\n",
    "        correction5 =correction5+self.regularItem*self.para['full_connect3_para_weight']\n",
    "\n",
    "        derivation_train,correction4,correction4_2 = full_connect_spread_backward(derivation_train,c4_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c3_relu_pack)\n",
    "        correction4 =correction4+self.regularItem*self.para['full_connect2_para_weight']\n",
    "\n",
    "        derivation_train,correction3,correction3_2 = full_connect_spread_backward(derivation_train,c3_pack)\n",
    "        derivation_train = pool_spread_backward(derivation_train,c2_pool_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c2_relu_pack)\n",
    "        correction3 =correction3+self.regularItem*self.para['full_connect1_para_weight']\n",
    "\n",
    "        derivation_train,correction2,correction2_2 = convolution_spread_backward(derivation_train,c2_pack)\n",
    "        derivation_train = pool_spread_backward(derivation_train,c1_pool_pack)\n",
    "        derivation_train = relu_spread_backward(derivation_train,c1_relu_pack)\n",
    "        correction2 =correction2+self.regularItem*self.para['convolution2_para_weight']\n",
    "\n",
    "        derivation_train,correction1,correction1_2 = convolution_spread_backward(derivation_train,c1_pack)\n",
    "        correction1 =correction1+self.regularItem*self.para['convolution1_para_weight']\n",
    "\n",
    "        corr = {}\n",
    "        corr['convolution1_para_weight'] = correction1\n",
    "        corr['convolution1_para_bias'] = correction1_2\n",
    "\n",
    "        corr['convolution2_para_weight'] = correction2\n",
    "        corr['convolution2_para_bias'] = correction2_2\n",
    "\n",
    "        corr['full_connect1_para_weight'] = correction3\n",
    "        corr['full_connect1_para_bias'] = correction3_2\n",
    "\n",
    "        corr['full_connect2_para_weight'] = correction4\n",
    "        corr['full_connect2_para_bias'] = correction4_2\n",
    "\n",
    "        corr['full_connect3_para_weight'] = correction5\n",
    "        corr['full_connect3_para_bias'] = correction5_2\n",
    "\n",
    "        return l,corr\n",
    "\n",
    "    def spread2(self,train):\n",
    "        # 卷积层步长为1，没有padding\n",
    "        convolution_para = {}\n",
    "        convolution_para.clear()\n",
    "        convolution_para['step']=1\n",
    "        convolution_para['pad']=0\n",
    "\n",
    "        # 池化层步长为2 范围就是【2,2】\n",
    "        pooling_para={}\n",
    "        pooling_para.clear()\n",
    "        pooling_para['step']=2\n",
    "        pooling_para['pool_height']=2\n",
    "        pooling_para['pool_wide']=2\n",
    "\n",
    "        # 拿出所有需要的权重\n",
    "        c1w = self.para['convolution1_para_weight']\n",
    "        temp1 = np.sum(c1w*c1w)\n",
    "        c1b = self.para['convolution1_para_bias']\n",
    "        c2w = self.para['convolution2_para_weight']\n",
    "        temp2 = np.sum(c2w*c2w)\n",
    "        c2b = self.para['convolution2_para_bias']\n",
    "        fc3w = self.para['full_connect1_para_weight']\n",
    "        temp3 = np.sum(fc3w*fc3w)\n",
    "        fc3b = self.para['full_connect1_para_bias']\n",
    "        fc4w = self.para['full_connect2_para_weight']\n",
    "        temp4 = np.sum(fc4w*fc4w)\n",
    "        fc4b = self.para['full_connect2_para_bias']\n",
    "        fc5w = self.para['full_connect3_para_weight']\n",
    "        temp5 = np.sum(fc5w*fc5w)\n",
    "        fc5b = self.para['full_connect3_para_bias']\n",
    "\n",
    "        cal_temp = temp1 + temp2 +temp3 +temp4 +temp5\n",
    "        cal_temp = (self.regularItem*cal_temp)/2\n",
    "        #开始前向传播\n",
    "\n",
    "        # 第一次卷积层+relu+第一层池化层\n",
    "        c1_res,c1_pack = convolution_spread_forward(train,c1w,c1b,convolution_para)\n",
    "\n",
    "        c1_relu_res,c1_relu_pack = relu_spread_froward(c1_res)\n",
    "\n",
    "        c1_pool_res,c1_pool_pack = pool_spread_forward(c1_relu_res,pooling_para)\n",
    "\n",
    "        # 第二次卷积层+relu+第二层池化层\n",
    "        c2_res,c2_pack = convolution_spread_forward(c1_pool_res,c2w,c2b,convolution_para)\n",
    "\n",
    "        c2_relu_res,c2_relu_pack = relu_spread_froward(c2_res)\n",
    "\n",
    "        c2_pool_res,c2_pool_pack = pool_spread_forward(c2_relu_res,pooling_para)\n",
    "\n",
    "        # 第一层全连接层+relu层\n",
    "        c3_res,c3_pack = full_connect_spread_forward(c2_pool_res,fc3w,fc3b)\n",
    "\n",
    "        c3_relu_res,c3_relu_pack = relu_spread_froward(c3_res)\n",
    "\n",
    "        # 第二层全连接层\n",
    "        c4_res,c4_pack = full_connect_spread_forward(c3_relu_res,fc4w,fc4b)\n",
    "\n",
    "        c4_relu_res,c4_relu_pack = relu_spread_froward(c4_res)\n",
    "\n",
    "        # 输出层\n",
    "        c5_out,c5_pack = full_connect_spread_forward(c4_relu_res,fc5w,fc5b)\n",
    "\n",
    "        return c5_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "魔数:2051, 图片数量: 60000张, 图片大小: 28*28\n",
      "本次解析的矩阵格式为[60000,28,28]\n",
      "魔数:2049, 图片数量: 60000张, 图片大小: 1*1\n",
      "本次解析的矩阵格式为[60000,1,1]\n",
      "魔数:2051, 图片数量: 10000张, 图片大小: 28*28\n",
      "本次解析的矩阵格式为[10000,28,28]\n",
      "魔数:2049, 图片数量: 10000张, 图片大小: 1*1\n",
      "本次解析的矩阵格式为[10000,1,1]\n",
      "加载完毕\n",
      "(59500, 1, 32, 32)\n",
      "(59500,)\n",
      "(500, 1, 32, 32)\n",
      "(500,)\n",
      "第 1 轮第 1 个batch中loss为 2.302616\n",
      "第 1 轮第 2 个batch中loss为 2.302496\n",
      "第 1 轮第 3 个batch中loss为 2.302552\n",
      "第 1 轮第 4 个batch中loss为 2.302674\n",
      "第 1 轮第 5 个batch中loss为 2.302406\n",
      "第 1 轮第 6 个batch中loss为 2.301984\n",
      "第 1 轮第 7 个batch中loss为 2.302518\n",
      "第 1 轮第 8 个batch中loss为 2.302697\n",
      "第 1 轮第 9 个batch中loss为 2.302375\n",
      "第 1 轮第 10 个batch中loss为 2.302429\n",
      "第 1 轮第 11 个batch中loss为 2.302580\n",
      "第 1 轮第 12 个batch中loss为 2.302394\n",
      "第 1 轮第 13 个batch中loss为 2.302341\n",
      "第 1 轮第 14 个batch中loss为 2.302119\n",
      "第 1 轮第 15 个batch中loss为 2.302067\n",
      "第 1 轮第 16 个batch中loss为 2.302861\n",
      "第 1 轮第 17 个batch中loss为 2.301529\n",
      "第 1 轮第 18 个batch中loss为 2.302358\n",
      "第 1 轮第 19 个batch中loss为 2.302536\n",
      "第 1 轮第 20 个batch中loss为 2.302092\n",
      "第 1 轮第 21 个batch中loss为 2.301750\n",
      "第 1 轮第 22 个batch中loss为 2.301883\n",
      "第 1 轮第 23 个batch中loss为 2.301239\n",
      "第 1 轮第 24 个batch中loss为 2.302263\n",
      "第 1 轮第 25 个batch中loss为 2.300414\n",
      "第 1 轮第 26 个batch中loss为 2.301408\n",
      "第 1 轮第 27 个batch中loss为 2.301861\n",
      "第 1 轮第 28 个batch中loss为 2.298743\n",
      "第 1 轮第 29 个batch中loss为 2.296206\n",
      "第 1 轮第 30 个batch中loss为 2.285414\n",
      "第 1 轮第 31 个batch中loss为 2.276633\n",
      "第 1 轮第 32 个batch中loss为 2.243157\n",
      "第 1 轮第 33 个batch中loss为 2.251971\n",
      "第 1 轮第 34 个batch中loss为 2.204739\n",
      "第 1 轮第 35 个batch中loss为 2.219951\n",
      "第 1 轮第 36 个batch中loss为 2.167482\n",
      "第 1 轮第 37 个batch中loss为 2.170142\n",
      "第 1 轮第 38 个batch中loss为 2.069762\n",
      "第 1 轮第 39 个batch中loss为 2.112798\n",
      "第 1 轮第 40 个batch中loss为 2.151788\n",
      "第 1 轮第 41 个batch中loss为 2.059514\n",
      "第 1 轮第 42 个batch中loss为 2.100846\n",
      "第 1 轮第 43 个batch中loss为 2.045083\n",
      "第 1 轮第 44 个batch中loss为 2.109001\n",
      "第 1 轮第 45 个batch中loss为 2.056799\n",
      "第 1 轮第 46 个batch中loss为 2.057324\n",
      "第 1 轮第 47 个batch中loss为 2.045031\n",
      "第 1 轮第 48 个batch中loss为 2.030317\n",
      "第 1 轮第 49 个batch中loss为 2.016372\n",
      "第 1 轮第 50 个batch中loss为 2.007091\n",
      "第 1 轮第 51 个batch中loss为 1.975228\n",
      "第 1 轮第 52 个batch中loss为 2.002439\n",
      "第 1 轮第 53 个batch中loss为 1.981245\n",
      "第 1 轮第 54 个batch中loss为 1.966341\n",
      "第 1 轮第 55 个batch中loss为 1.948170\n",
      "第 1 轮第 56 个batch中loss为 1.918643\n",
      "第 1 轮第 57 个batch中loss为 1.902277\n",
      "第 1 轮第 58 个batch中loss为 1.947061\n",
      "第 1 轮第 59 个batch中loss为 1.946001\n",
      "第 1 轮第 60 个batch中loss为 1.961033\n",
      "第 1 轮第 61 个batch中loss为 1.912796\n",
      "第 1 轮第 62 个batch中loss为 1.923781\n",
      "第 1 轮第 63 个batch中loss为 1.880870\n",
      "第 1 轮第 64 个batch中loss为 1.877339\n",
      "第 1 轮第 65 个batch中loss为 1.867155\n",
      "第 1 轮第 66 个batch中loss为 1.882186\n",
      "第 1 轮第 67 个batch中loss为 1.869756\n",
      "第 1 轮第 68 个batch中loss为 1.799705\n",
      "第 1 轮第 69 个batch中loss为 1.767340\n",
      "第 1 轮第 70 个batch中loss为 1.734021\n",
      "第 1 轮第 71 个batch中loss为 1.798214\n",
      "第 1 轮第 72 个batch中loss为 1.787524\n",
      "第 1 轮第 73 个batch中loss为 1.770145\n",
      "第 1 轮第 74 个batch中loss为 1.932100\n",
      "第 1 轮第 75 个batch中loss为 1.692426\n",
      "第 1 轮第 76 个batch中loss为 1.697176\n",
      "第 1 轮第 77 个batch中loss为 1.652174\n",
      "第 1 轮第 78 个batch中loss为 1.654569\n",
      "第 1 轮第 79 个batch中loss为 1.633251\n",
      "第 1 轮第 80 个batch中loss为 1.580804\n",
      "第 1 轮第 81 个batch中loss为 1.557516\n",
      "第 1 轮第 82 个batch中loss为 1.514791\n",
      "第 1 轮第 83 个batch中loss为 1.570504\n",
      "第 1 轮第 84 个batch中loss为 1.515040\n",
      "第 1 轮第 85 个batch中loss为 1.392149\n",
      "第 1 轮第 86 个batch中loss为 1.545936\n",
      "第 1 轮第 87 个batch中loss为 1.562114\n",
      "第 1 轮第 88 个batch中loss为 1.472743\n",
      "第 1 轮第 89 个batch中loss为 1.428257\n",
      "第 1 轮第 90 个batch中loss为 1.385207\n",
      "第 1 轮第 91 个batch中loss为 1.377703\n",
      "第 1 轮第 92 个batch中loss为 1.481915\n",
      "第 1 轮第 93 个batch中loss为 1.400101\n",
      "第 1 轮第 94 个batch中loss为 1.507118\n",
      "第 1 轮第 95 个batch中loss为 1.281754\n",
      "第 1 轮第 96 个batch中loss为 1.461928\n",
      "第 1 轮第 97 个batch中loss为 1.341426\n",
      "第 1 轮第 98 个batch中loss为 1.297625\n",
      "第 1 轮第 99 个batch中loss为 1.249926\n",
      "第 1 轮第 100 个batch中loss为 1.261299\n",
      "第 1 轮第 101 个batch中loss为 1.387863\n",
      "第 1 轮第 102 个batch中loss为 1.279199\n",
      "第 1 轮第 103 个batch中loss为 1.194600\n",
      "第 1 轮第 104 个batch中loss为 1.353252\n",
      "第 1 轮第 105 个batch中loss为 1.150883\n",
      "第 1 轮第 106 个batch中loss为 1.051460\n",
      "第 1 轮第 107 个batch中loss为 1.083936\n",
      "第 1 轮第 108 个batch中loss为 1.088020\n",
      "第 1 轮第 109 个batch中loss为 0.997441\n",
      "第 1 轮第 110 个batch中loss为 0.948988\n",
      "第 1 轮第 111 个batch中loss为 0.988235\n",
      "第 1 轮第 112 个batch中loss为 0.994085\n",
      "第 1 轮第 113 个batch中loss为 1.002342\n",
      "第 1 轮第 114 个batch中loss为 0.993505\n",
      "第 1 轮第 115 个batch中loss为 0.778338\n",
      "第 1 轮第 116 个batch中loss为 0.863077\n",
      "第 1 轮第 117 个batch中loss为 0.696048\n",
      "第 1 轮第 118 个batch中loss为 0.767842\n",
      "第 1 轮第 119 个batch中loss为 0.760502\n",
      "第 1 轮第 120 个batch中loss为 0.663058\n",
      "第 1 轮第 121 个batch中loss为 0.851743\n",
      "第 1 轮第 122 个batch中loss为 0.784473\n",
      "第 1 轮第 123 个batch中loss为 0.679074\n",
      "第 1 轮第 124 个batch中loss为 0.824409\n",
      "第 1 轮第 125 个batch中loss为 0.666960\n",
      "第 1 轮第 126 个batch中loss为 0.593295\n",
      "第 1 轮第 127 个batch中loss为 0.574982\n",
      "第 1 轮第 128 个batch中loss为 0.642305\n",
      "第 1 轮第 129 个batch中loss为 0.589595\n",
      "第 1 轮第 130 个batch中loss为 0.689868\n",
      "第 1 轮第 131 个batch中loss为 0.483302\n",
      "第 1 轮第 132 个batch中loss为 0.620814\n",
      "第 1 轮第 133 个batch中loss为 0.668583\n",
      "第 1 轮第 134 个batch中loss为 0.764551\n",
      "第 1 轮第 135 个batch中loss为 0.484416\n",
      "第 1 轮第 136 个batch中loss为 0.684098\n",
      "第 1 轮第 137 个batch中loss为 0.579408\n",
      "第 1 轮第 138 个batch中loss为 0.634654\n",
      "第 1 轮第 139 个batch中loss为 0.643033\n",
      "第 1 轮第 140 个batch中loss为 0.501734\n",
      "第 1 轮第 141 个batch中loss为 0.422528\n",
      "第 1 轮第 142 个batch中loss为 0.680839\n",
      "第 1 轮第 143 个batch中loss为 0.492999\n",
      "第 1 轮第 144 个batch中loss为 0.788878\n",
      "第 1 轮第 145 个batch中loss为 0.444428\n",
      "第 1 轮第 146 个batch中loss为 0.564387\n",
      "第 1 轮第 147 个batch中loss为 0.577255\n",
      "第 1 轮第 148 个batch中loss为 0.397459\n",
      "第 1 轮第 149 个batch中loss为 0.447091\n",
      "第 1 轮第 150 个batch中loss为 0.554031\n",
      "第 1 轮第 151 个batch中loss为 0.562941\n",
      "第 1 轮第 152 个batch中loss为 0.307689\n",
      "第 1 轮第 153 个batch中loss为 0.481083\n",
      "第 1 轮第 154 个batch中loss为 0.471620\n",
      "第 1 轮第 155 个batch中loss为 0.439860\n",
      "第 1 轮第 156 个batch中loss为 0.448688\n",
      "第 1 轮第 157 个batch中loss为 0.529589\n",
      "第 1 轮第 158 个batch中loss为 0.401130\n",
      "第 1 轮第 159 个batch中loss为 0.474762\n",
      "第 1 轮第 160 个batch中loss为 0.224265\n",
      "第 1 轮第 161 个batch中loss为 0.476856\n",
      "第 1 轮第 162 个batch中loss为 0.370568\n",
      "第 1 轮第 163 个batch中loss为 0.379159\n",
      "第 1 轮第 164 个batch中loss为 0.322032\n",
      "第 1 轮第 165 个batch中loss为 0.318278\n",
      "第 1 轮第 166 个batch中loss为 0.274938\n",
      "第 1 轮第 167 个batch中loss为 0.389008\n",
      "第 1 轮第 168 个batch中loss为 0.253071\n",
      "第 1 轮第 169 个batch中loss为 0.278644\n",
      "第 1 轮第 170 个batch中loss为 0.175217\n",
      "第 1 轮第 171 个batch中loss为 0.319373\n",
      "第 1 轮第 172 个batch中loss为 0.290681\n",
      "第 1 轮第 173 个batch中loss为 0.330946\n",
      "第 1 轮第 174 个batch中loss为 0.331674\n",
      "第 1 轮第 175 个batch中loss为 0.304227\n",
      "第 1 轮第 176 个batch中loss为 0.226337\n",
      "第 1 轮第 177 个batch中loss为 0.222670\n",
      "第 1 轮第 178 个batch中loss为 0.231505\n",
      "第 1 轮第 179 个batch中loss为 0.276633\n",
      "第 1 轮第 180 个batch中loss为 0.195927\n",
      "第 1 轮第 181 个batch中loss为 0.349251\n",
      "第 1 轮第 182 个batch中loss为 0.146176\n",
      "第 1 轮第 183 个batch中loss为 0.187348\n",
      "第 1 轮第 184 个batch中loss为 0.229019\n",
      "第 1 轮第 185 个batch中loss为 0.231680\n",
      "第 1 轮第 186 个batch中loss为 0.257546\n",
      "第 1 轮第 187 个batch中loss为 0.343903\n",
      "第 1 轮第 188 个batch中loss为 0.486494\n",
      "第 1 轮第 189 个batch中loss为 0.198838\n",
      "第 1 轮第 190 个batch中loss为 0.170056\n",
      "第 1 轮第 191 个batch中loss为 0.179699\n",
      "第 1 轮第 192 个batch中loss为 0.273973\n",
      "第 1 轮第 193 个batch中loss为 0.210955\n",
      "第 1 轮第 194 个batch中loss为 0.349444\n",
      "第 1 轮第 195 个batch中loss为 0.214936\n",
      "第 1 轮第 196 个batch中loss为 0.193883\n",
      "第 1 轮第 197 个batch中loss为 0.360000\n",
      "第 1 轮第 198 个batch中loss为 0.228365\n",
      "第 1 轮第 199 个batch中loss为 0.279079\n",
      "第 1 轮第 200 个batch中loss为 0.243329\n",
      "第 1 轮第 201 个batch中loss为 0.159326\n",
      "第 1 轮第 202 个batch中loss为 0.227827\n",
      "第 1 轮第 203 个batch中loss为 0.223446\n",
      "第 1 轮第 204 个batch中loss为 0.219060\n",
      "第 1 轮第 205 个batch中loss为 0.191183\n",
      "第 1 轮第 206 个batch中loss为 0.202820\n",
      "第 1 轮第 207 个batch中loss为 0.309844\n",
      "第 1 轮第 208 个batch中loss为 0.369402\n",
      "第 1 轮第 209 个batch中loss为 0.173480\n",
      "第 1 轮第 210 个batch中loss为 0.246692\n",
      "第 1 轮第 211 个batch中loss为 0.252630\n",
      "第 1 轮第 212 个batch中loss为 0.227413\n",
      "第 1 轮第 213 个batch中loss为 0.277470\n",
      "第 1 轮第 214 个batch中loss为 0.213667\n",
      "第 1 轮第 215 个batch中loss为 0.287805\n",
      "第 1 轮第 216 个batch中loss为 0.243460\n",
      "第 1 轮第 217 个batch中loss为 0.143504\n",
      "第 1 轮第 218 个batch中loss为 0.152889\n",
      "第 1 轮第 219 个batch中loss为 0.141539\n",
      "第 1 轮第 220 个batch中loss为 0.268312\n",
      "第 1 轮第 221 个batch中loss为 0.195796\n",
      "第 1 轮第 222 个batch中loss为 0.260097\n",
      "第 1 轮第 223 个batch中loss为 0.158107\n",
      "第 1 轮第 224 个batch中loss为 0.176606\n",
      "第 1 轮第 225 个batch中loss为 0.276858\n",
      "第 1 轮第 226 个batch中loss为 0.241572\n",
      "第 1 轮第 227 个batch中loss为 0.104195\n",
      "第 1 轮第 228 个batch中loss为 0.291983\n",
      "第 1 轮第 229 个batch中loss为 0.281165\n",
      "第 1 轮第 230 个batch中loss为 0.319187\n",
      "第 1 轮第 231 个batch中loss为 0.219841\n",
      "第 1 轮第 232 个batch中loss为 0.412676\n",
      "第 1 轮第 233 个batch中loss为 0.212521\n",
      "第 1 轮第 234 个batch中loss为 0.165882\n",
      "第 1 轮第 235 个batch中loss为 0.158611\n",
      "第 1 轮第 236 个batch中loss为 0.219946\n",
      "第 1 轮第 237 个batch中loss为 0.354495\n",
      "第 1 轮第 238 个batch中loss为 0.253801\n",
      "第 1 轮第 239 个batch中loss为 0.183191\n",
      "第 1 轮第 240 个batch中loss为 0.219854\n",
      "第 1 轮第 241 个batch中loss为 0.153933\n",
      "第 1 轮第 242 个batch中loss为 0.223270\n",
      "第 1 轮第 243 个batch中loss为 0.150133\n",
      "第 1 轮第 244 个batch中loss为 0.100904\n",
      "第 1 轮第 245 个batch中loss为 0.235587\n",
      "第 1 轮第 246 个batch中loss为 0.267165\n",
      "第 1 轮第 247 个batch中loss为 0.104108\n",
      "第 1 轮第 248 个batch中loss为 0.330046\n",
      "第 1 轮第 249 个batch中loss为 0.171418\n",
      "第 1 轮第 250 个batch中loss为 0.274806\n",
      "第 1 轮第 251 个batch中loss为 0.141670\n",
      "第 1 轮第 252 个batch中loss为 0.303249\n",
      "第 1 轮第 253 个batch中loss为 0.204155\n",
      "第 1 轮第 254 个batch中loss为 0.205060\n",
      "第 1 轮第 255 个batch中loss为 0.183702\n",
      "第 1 轮第 256 个batch中loss为 0.143786\n",
      "第 1 轮第 257 个batch中loss为 0.195831\n",
      "第 1 轮第 258 个batch中loss为 0.099123\n",
      "第 1 轮第 259 个batch中loss为 0.149339\n",
      "第 1 轮第 260 个batch中loss为 0.150933\n",
      "第 1 轮第 261 个batch中loss为 0.257637\n",
      "第 1 轮第 262 个batch中loss为 0.190416\n",
      "第 1 轮第 263 个batch中loss为 0.147282\n",
      "第 1 轮第 264 个batch中loss为 0.153303\n",
      "第 1 轮第 265 个batch中loss为 0.289695\n",
      "第 1 轮第 266 个batch中loss为 0.148337\n",
      "第 1 轮第 267 个batch中loss为 0.124332\n",
      "第 1 轮第 268 个batch中loss为 0.139122\n",
      "第 1 轮第 269 个batch中loss为 0.166573\n",
      "第 1 轮第 270 个batch中loss为 0.175667\n",
      "第 1 轮第 271 个batch中loss为 0.175282\n",
      "第 1 轮第 272 个batch中loss为 0.136278\n",
      "第 1 轮第 273 个batch中loss为 0.178948\n",
      "第 1 轮第 274 个batch中loss为 0.097908\n",
      "第 1 轮第 275 个batch中loss为 0.159383\n",
      "第 1 轮第 276 个batch中loss为 0.102170\n",
      "第 1 轮第 277 个batch中loss为 0.146161\n",
      "第 1 轮第 278 个batch中loss为 0.146656\n",
      "第 1 轮第 279 个batch中loss为 0.160712\n",
      "第 1 轮第 280 个batch中loss为 0.136543\n",
      "第 1 轮第 281 个batch中loss为 0.083651\n",
      "第 1 轮第 282 个batch中loss为 0.152879\n",
      "第 1 轮第 283 个batch中loss为 0.146279\n",
      "第 1 轮第 284 个batch中loss为 0.097647\n",
      "第 1 轮第 285 个batch中loss为 0.120825\n",
      "第 1 轮第 286 个batch中loss为 0.099112\n",
      "第 1 轮第 287 个batch中loss为 0.114144\n",
      "第 1 轮第 288 个batch中loss为 0.155891\n",
      "第 1 轮第 289 个batch中loss为 0.209368\n",
      "第 1 轮第 290 个batch中loss为 0.101736\n",
      "第 1 轮第 291 个batch中loss为 0.109471\n",
      "第 1 轮第 292 个batch中loss为 0.062839\n",
      "第 1 轮第 293 个batch中loss为 0.122459\n",
      "第 1 轮第 294 个batch中loss为 0.041894\n",
      "第 1 轮第 295 个batch中loss为 0.141338\n",
      "第 1 轮第 296 个batch中loss为 0.026264\n",
      "第 1 轮第 297 个batch中loss为 0.245827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\TempFile\\ipykernel_21368\\2183595148.py:60: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  return np.mean(prediction == label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 次迭代，训练集正确率 0.000000 验证集正确率 0.976000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MyLenet5' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [785]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\u001B[39;00m\n\u001B[0;32m     20\u001B[0m myTrain \u001B[38;5;241m=\u001B[39m Train(Lenet5\u001B[38;5;241m=\u001B[39mTrainModel,data\u001B[38;5;241m=\u001B[39mdata,epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m,parameter\u001B[38;5;241m=\u001B[39mmyParameter,decrease\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.75\u001B[39m,batchSize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m,exp\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 21\u001B[0m \u001B[43mmyTrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmyTrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [774]\u001B[0m, in \u001B[0;36mTrain.myTrain\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbestAccurate \u001B[38;5;241m=\u001B[39m b\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbestPara\u001B[38;5;241m.\u001B[39mclear() \u001B[38;5;66;03m#更新自己参数里的最佳参数\u001B[39;00m\n\u001B[1;32m---> 95\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k,v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m     96\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbestPara[k] \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'MyLenet5' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "# 调用接口，完成数据加载并打包\n",
    "trainImages,trainLabels,proofImages,proofLabels,testImages,testLabels = load_minist_data()\n",
    "data = {}\n",
    "data['train1']=trainImages\n",
    "data['train2']=proofImages\n",
    "data['label1']=trainLabels\n",
    "data['label2']=proofLabels\n",
    "data['test_image']=testImages\n",
    "data['test_label']=testLabels\n",
    "print(data['train1'].shape)\n",
    "print(data['label1'].shape)\n",
    "print(data['train2'].shape)\n",
    "print(data['label2'].shape)\n",
    "\n",
    "\n",
    "TrainModel = MyLenet5()\n",
    "myParameter = {}\n",
    "myParameter['learn_rate']=1e-3\n",
    "# def __init__(self, Lenet5, data, epoch, func, parameter, decrease, batchSize, exp):\n",
    "myTrain = Train(Lenet5=TrainModel,data=data,epoch=10,func='adam',parameter=myParameter,decrease=0.75,batchSize=200,exp=1)\n",
    "myTrain.myTrain()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
